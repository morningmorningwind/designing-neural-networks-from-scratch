{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入模块\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "from torch import nn\n",
    "import re\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data summary:\n",
      "\n",
      " number of poems: 2761\n",
      " number of words: 7651\n",
      "\n",
      "Poem examples:\n",
      "\n",
      "S四时运灰琯，一夕变冬春。送寒余雪尽，迎岁早梅新。E\n",
      "S上弦明月半，激箭流星远。落雁带书惊，啼猿映枝转。E\n",
      "S初秋玉露清，早雁出空鸣。隔云时乱影，因风乍含声。E\n",
      "S岸曲丝阴聚，波移带影疏。还将眉里翠，来就镜中舒。E\n",
      "S贞条障曲砌，翠叶贯寒霜。拂牖分龙影，临池待凤翔。E\n",
      "S散影玉阶柳，含翠隐鸣蝉。微形藏叶里，乱响出风前。E\n",
      "S盘根直盈渚，交干横倚天。舒华光四海，卷叶荫三川。E\n",
      "S近谷交萦蕊，遥峰对出莲。径细无全磴，松小未含烟。E\n",
      "S疾风知劲草，板荡识诚臣。勇夫安识义，智者必怀仁。E\n",
      "S太液仙舟迥，西园隐上才。未晓征车度，鸡鸣关早开。E\n"
     ]
    }
   ],
   "source": [
    "# 训练一个基于ERNN神经网络来作诗\n",
    "\n",
    "## 读入用GloVe处理得到的文字 embeddings，以及句子数据。\n",
    "import codecs\n",
    "\n",
    "word_emb_dim = input_size = 1\n",
    "i2w = {0:''}\n",
    "w2i = {'':0}\n",
    "\n",
    "word_emb_dim = 128\n",
    "\n",
    "with codecs.open('data/word_embeddings_128.txt', mode='r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    n_words = len(lines)+1\n",
    "    word_embeddings = torch.nn.Embedding(n_words, word_emb_dim)\n",
    "    for i in range(1, n_words):\n",
    "        line = lines[i-1].split(' ')\n",
    "        i2w[i] = line[0]\n",
    "        w2i[line[0]] = i\n",
    "        word_embeddings.weight[i] = torch.from_numpy(np.array(line[1:],dtype=np.float32))\n",
    "\n",
    "max_line_length = 26\n",
    "poems = []\n",
    "with codecs.open('data/poems.txt', mode='r', encoding='utf-8') as f:\n",
    "    for poem in f:\n",
    "        poem = re.sub('\\s','',poem)\n",
    "        poem = poem.split(':')[-1]\n",
    "        poem = 'S'+poem+'E'\n",
    "        if len(poem) < 10 or len(poem) > max_line_length or '(' in poem or u'（' in poem or u'《' in poem or '-' in poem or '_' in poem:\n",
    "            continue\n",
    "#        poem = re.split(u'[。；.?？]', poem)\n",
    "#        for s in poem:\n",
    "#            if len(s)>3:\n",
    "#                s += u'。'\n",
    "        poems.append(map(w2i.get, poem))\n",
    "\n",
    "n_poems = len(poems)\n",
    "\n",
    "print( 'Data summary:\\n\\n number of poems: {}\\n number of words: {}\\n'.format(n_poems, n_words))\n",
    "print('Poem examples:\\n\\n'+'\\n'.join([''.join(map(i2w.get, x)) for x in poems[:10]]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S独酌芳春酒，登楼已半曛。谁惊一行雁，冲断过江云。\n",
      "独酌芳春酒，登楼已半曛。谁惊一行雁，冲断过江云。E\n"
     ]
    }
   ],
   "source": [
    "# 定义一个函数，随机返回一个 mini batch，用于训练，由于每一首诗歌的长度不同，我们此处规定每个batch只有一首诗。这样，就可以生成长度可变的诗歌。\n",
    "def get_batch(batch_size=2):\n",
    "    batch_raw = [poems[i][:] for i in np.random.randint(0, n_poems, batch_size)]\n",
    "    max_length = max(map(len, batch_raw))\n",
    "    for i in range(len(batch_raw)):\n",
    "        for j in range(len(batch_raw[i]),max_length):\n",
    "            batch_raw[i].append(0)\n",
    "    batch_raw = torch.LongTensor(batch_raw).detach().unsqueeze(2).transpose(0,1)\n",
    "    x = batch_raw[:-1].type(torch.float32)\n",
    "    y = batch_raw[1:]\n",
    "    return x, y\n",
    "\n",
    "def idx2emb(x):\n",
    "    return word_embeddings(x.type(torch.long)).squeeze(2).detach()\n",
    "    \n",
    "\n",
    "# 定义一个函数，输入一个 batch 返回句子\n",
    "def batch2sent(batch):\n",
    "    S = []\n",
    "    batch = batch.type(torch.int32).detach()\n",
    "    seq_length, batch_size, emb_size = batch.size()\n",
    "    for i in range(batch_size):\n",
    "        S.append(''.join(map(i2w.get, batch[:,i,:].view(-1).tolist())))\n",
    "    return u'\\n'.join(S)\n",
    "\n",
    "x, y = get_batch(1)\n",
    "print(batch2sent(x))\n",
    "print(batch2sent(y))\n",
    "\n",
    "# 定义一个生成器\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size, n_layers=2, activation=None):\n",
    "        super(Generator, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.activation = activation\n",
    "        self.rnn = nn.LSTM(self.input_size, self.hidden_size, num_layers=self.n_layers, dropout=0.01)\n",
    "        self.output = nn.Linear(self.hidden_size,self.output_size)\n",
    "        self.softmax = torch.nn.LogSoftmax(dim=-1)\n",
    "    def init_h(self):\n",
    "        return (torch.zeros(self.n_layers, self.batch_size, self.hidden_size),torch.zeros(self.n_layers, self.batch_size, self.hidden_size))\n",
    "    def forward(self, x, h0=None):\n",
    "        self.seq_length, self.batch_size, self.input_size = x.size()\n",
    "        if h0 is None:\n",
    "            h0 = self.init_h()\n",
    "#            x0 = torch.FloatTensor([w2i['S']]).view(1,1,-1).detach()\n",
    "#            x0 = idx2emb(x0)\n",
    "#            y0, h0 = self.rnn(x0,h0)\n",
    "        y, ht = self.rnn(x,h0)\n",
    "#        y = torch.cat((y0,y),dim=0)\n",
    "        y = y.view(-1,self.hidden_size)\n",
    "        y = self.output(y)\n",
    "        y = y.view(self.seq_length,self.batch_size,self.output_size)\n",
    "        y = self.softmax(y)\n",
    "        return y, ht\n",
    "\n",
    "def poem_gen(model, w=None):\n",
    "    with torch.no_grad():\n",
    "        if not w in w2i or w is None:\n",
    "            idx = np.random.randint(1,n_words)\n",
    "            w = i2w[idx]\n",
    "        else:\n",
    "            idx = w2i[w]\n",
    "        ht = None\n",
    "        x0 = torch.FloatTensor([w2i['S']]).view(1,1,-1).detach()\n",
    "        x0 = idx2emb(x0)\n",
    "        y, ht = model(x0, ht)\n",
    "        x = torch.FloatTensor([w2i[w]]).view(1,1,-1).detach()\n",
    "        x = idx2emb(x)\n",
    "\n",
    "        s = ['S']\n",
    "        s.append(w)\n",
    "        for t in range(max_line_length):\n",
    "            y, ht = model(x, ht)\n",
    "            x = torch.argmax(y, dim=2, keepdim=True)\n",
    "            w = batch2sent(x)\n",
    "            s.append(w)\n",
    "            x = idx2emb(x)\n",
    "            if w == '':\n",
    "                break\n",
    "        return u''.join(s)\n",
    "    \n",
    "    \n",
    "# 训练一个简单的 RNN 模型以生成诗歌\n",
    "\n",
    "input_size = word_emb_dim\n",
    "hidden_size = 128\n",
    "output_size = n_words\n",
    "activation = torch.relu\n",
    "\n",
    "model = Generator(input_size, output_size, hidden_size, n_layers=2, activation=activation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch0, Loss3.7334856987\n",
      "Pred:\n",
      "一得不相人，无中似堪湿。春风如花湿，风人月衣心。E\n",
      "Obs:\n",
      "巧笑解迎人，晴雪香堪惜。随风蝶影翻，误点朝衣赤。E\n",
      "Rnd:\n",
      "S岁暮春风雨，一家一里西。殷勤E上月，不见去人。E君无得\n",
      "\n",
      "Epoch50, Loss2.96560144424\n",
      "Pred:\n",
      "一有不中下，清泉水不可。何怜谯记内，山水对禅人。E\n",
      "Obs:\n",
      "许厕高斋唱，涓泉定不如。可怜谯记室，流水满禅居。E\n",
      "Rnd:\n",
      "S觱木不堕，王王韦也。E君君不得，不得相人。E君无人事，\n",
      "\n",
      "Epoch100, Loss3.53442716599\n",
      "Pred:\n",
      "一南千何远，孤风入气里。他时何林里，寒里万边枝。E\n",
      "Obs:\n",
      "江旷春潮白，山长晓岫青。他乡临睨极，花柳映边亭。E\n",
      "Rnd:\n",
      "S蹀断凭悠悠，秋风入夜深。不怜西上月，偏照不知归。E山月\n",
      "\n",
      "Epoch150, Loss4.09891700745\n",
      "Pred:\n",
      "一门花朝伴，春风意不春。不是一月人，不君不生春。E\n",
      "Obs:\n",
      "玉英期共采，云岭独先过。应得灵芝也，诗情一倍多。E\n",
      "Rnd:\n",
      "S蓏汩几时夜，孤山月秋深。不知同人事，不见旧时时。E君无\n",
      "\n",
      "Epoch200, Loss3.83963871002\n",
      "Pred:\n",
      "一马三，里，三蝉力无城。何知三上去，不得不人山。E\n",
      "Obs:\n",
      "仗剑行千里，微躯感一言。曾为大梁客，不负信陵恩。E\n",
      "Rnd:\n",
      "S幻马不出，富心不动。E上有在树，一路始天人。E上无见去\n",
      "\n",
      "Epoch250, Loss3.07987070084\n",
      "Pred:\n",
      "一年无，树，一吟入未归。可怜西郭酒，偏风不家山。E\n",
      "Obs:\n",
      "十夜郡城宿，苦吟身未闲。那堪西郭别，雪路问青山。E\n",
      "Rnd:\n",
      "S孺马砺，薛不不薛。E南南望，日日半中。E上南南树，南南\n",
      "\n",
      "Epoch300, Loss3.09251761436\n",
      "Pred:\n",
      "三南孟风寂，山声出景声。不知花郭树，日E何悠重。E\n",
      "Obs:\n",
      "南楼夜已寂，暗鸟动林间。不见城郭事，沉沉唯四山。E\n",
      "Rnd:\n",
      "S襞鶒几无夜，孤中满月明。E君君不得见，不见鹔鹴裘。EE\n",
      "\n",
      "Epoch350, Loss3.64821505547\n",
      "Pred:\n",
      "一年三山远，不君在自迟。可君无年事，不是不中人。E\n",
      "Obs:\n",
      "一见嵩山老，吾生恨太迟。问君年几许，曾出上皇时。E\n",
      "Rnd:\n",
      "S锱豸自霜尽，龙履真霞尘。一来千里里，一取一家家。EE不\n",
      "\n",
      "Epoch400, Loss2.99710440636\n",
      "Pred:\n",
      "一牛朝月月，孤中夜云林。不知知齿尽，不有有中生。E\n",
      "Obs:\n",
      "银地秋月色，石梁夜溪声。谁知屐齿尽，为破烟苔行。E\n",
      "Rnd:\n",
      "S鼾及富，薛子薛笔。EE江风吹急急，风风皓景秋。不知同人\n",
      "\n",
      "Epoch450, Loss3.31048512459\n",
      "Pred:\n",
      "一石花花枝，花叶月流流。不逢人不见，不是不莲人。E\n",
      "Obs:\n",
      "玉溆花争发，金塘水乱流。相逢畏相失，并著采莲舟。E\n",
      "Rnd:\n",
      "S纛及借薛，后后薛价。EE江水清水，日暮绒阳楼。E君君不\n",
      "\n",
      "Epoch500, Loss3.63501858711\n",
      "Pred:\n",
      "一人黄株树，花见别南宫。今台无月事，不日不南悠。E\n",
      "Obs:\n",
      "路傍一株柳，此路向延州。延州在何处，此路起悠悠。E\n",
      "Rnd:\n",
      "S斥如今年，王后韦也。EE江水南南，日暮管弦弦。E望风风\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-1013e0b7a3cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0my_obs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_obs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_obs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mLoss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/torch/tensor.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m \u001b[0;34m`\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/torch/autograd/__init__.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lr = 1e-3\n",
    "n_epochs = 10000\n",
    "last_epoch = -1\n",
    "disp_interval = 50\n",
    "batch_size = 1\n",
    "\n",
    "loss_func = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n",
    "\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "def lr_lambda(epoch):\n",
    "    return 0.99**(epoch/50.0)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch=last_epoch)\n",
    "\n",
    "model.load_state_dict(torch.load('saves/model.pt'))\n",
    "\n",
    "Loss = []\n",
    "for epoch in range(n_epochs):\n",
    "    model.zero_grad()\n",
    "    x_obs, y_obs = get_batch(batch_size=batch_size)\n",
    "    x_obs = idx2emb(x_obs)\n",
    "    y_pred, ht = model(x_obs)\n",
    "    y1 = torch.argmax(y_pred.detach(),-1,keepdim=True).detach()#[:,:1,:]\n",
    "    y2 = y_obs.detach()#[:,:1,:]\n",
    "    y_pred = y_pred.view(-1,output_size)\n",
    "    y_obs = y_obs.contiguous().view(-1)\n",
    "    loss = loss_func(y_pred,y_obs)\n",
    "    loss.backward()\n",
    "    Loss.append(loss.tolist())\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    if epoch % disp_interval == 0:\n",
    "        print(u'Epoch{}, Loss{}\\nPred:\\n{}\\nObs:\\n{}\\nRnd:\\n{}\\n'.format(epoch,loss.tolist(), batch2sent(y1), batch2sent(y2),poem_gen(model)))\n",
    "        torch.save(model.state_dict(),'saves/model.pt')\n",
    "window_size = 50\n",
    "avg_losses = np.array(Loss)[:len(Loss)//50 *50].reshape([-1,window_size]).mean(1)\n",
    "pl.plot(np.arange(0,len(Loss)//50 *50,window_size), avg_losses,'r-')\n",
    "pl.xlabel('Time')\n",
    "pl.ylabel('Loss')\n",
    "pl.yscale('log')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = get_batch(batch_size=2)\n",
    "print x.size()\n",
    "print x[:,0,:], y[:,0,:]\n",
    "print x[:,1,:], y[:,1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pred.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.size(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2,3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = nn.LSTM(4, 6, 2)\n",
    "x = np.random.randn(2, 5, 4)\n",
    "for i in range(10):\n",
    "    input = torch.from_numpy(x).type(torch.float32)\n",
    "    input=input.t()\n",
    "    rnn.zero_grad()\n",
    "    output, (hn, cn) = rnn(input)\n",
    "    print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.transpose(input,0,1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25, 5, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_obs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.transpose(input,1,0).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.topk(torch.randn(1,1,100),1,-1)[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

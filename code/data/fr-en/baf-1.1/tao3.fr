Réaccentuation automatique 
de textes français

Michel Simard

simard@citi.doc.ca

Industrie Canada
Centre d'Innovation en Technologies de l'Information
1575 Chomedey
Laval (Québec)
CANADA H7V 2X2

1	Introduction

Les travaux présentés ici s'inscrivent dans le cadre du projet Robustesse, mené par l'équipe de 
traduction assistée par ordinateur (TAO) du CITI. Ce projet vise à élaborer des méthodes et des 
outils de traitement des langues naturelles robustes : plusieurs systèmes de TALN vont soit refuser 
de traiter des textes comportant des erreurs ou des phénomènes étrangers à leur propre ensemble 
de connaissances, soit afficher un comportement imprévisible dans ces circonstances. À l'opposé, 
un système robuste se comportera alors de façon prévisible et utile.

Les textes français sans accents (marques diacritiques) constituent un exemple typique et 
particulièrement répandu des problèmes auxquels font face les systèmes de TALN. C'est dans le 
contexte du courrier électronique (e-mail) qu'on rencontre le plus souvent ce phénomène, qui 
s'explique de deux façons. Premièrement, le monde de l'informatique a longtemps souffert de 
l'absence d'une norme suffisamment répandue pour l'encodage des caractères accentués, ce qui 
a entraîné toute une panoplie de problèmes de transfert et de traitement des texte français. Il n'est 
d'ailleurs pas rare qu'un des maillons logiciels dans la chaîne de distribution du courrier 
électronique "désaccentue" délibérément les caractères accentués, afin de prévenir d'éventuels 
problèmes. Deuxièmement, la saisie au clavier des caractères accentués demeure, encore à ce 
jour, un exercice ardu, voire acrobatique dans certains cas : ici, il s'agit à la fois d'une question de 
norme et d'une question d'ergonomie. Le résultat concret, c'est qu'un très grand nombre 
d'utilisateurs francophones évite systématiquement d'utiliser les caractères accentués, tout du 
moins pour le courrier électronique.

Si cette situation demeure tolérable en pratique, c'est parce qu'il est extrêmement rare que la 
disparition des accents rende un texte français incompréhensible pour un humain. D'un point de 
vue linguistique, l'absence d'accents en français ne fait qu'augmenter le degré relatif d'ambiguïté 
inhérent à la langue. À la limite, elle ralentit la lecture et suscite un certain inconfort, comme peut 
le faire, par exemple, la lecture d'un texte rédigé entièrement en majuscules.

Il n'en demeure pas moins que si le français sans accent est acceptable dans certaines 
circonstances, il ne l'est pas dans l'usage courant, notamment dans le cas des documents 
imprimés. Par ailleurs, l'absence des accents pose de sérieux problèmes pour le traitement 
automatique des textes. Qu'il s'agisse de recherche documentaire, de correction orthographique, 
grammaticale, stylistique, de traduction automatique, d'interface en langue en naturelle ou de 
quelqu'autre forme de traitement de la langue, on s'attendra en général à ce que les textes français 
comportent des accents. D'où l'intérêt pour des méthodes de récupération automatique des 
accents, ou de réaccentuation automatique.

En examinant le problème, on constate que la grande majorité des mots d'un texte français 
s'écrivent naturellement sans accents (envrion 85%), et que pour plus de la moitié des mots qui 
restent, la forme accentuée correcte peut être déduite de façon déterministe à partir de la forme 
sans accent. Il en découle que la simple utilisation d'un bon dictionnaire permet de réaccentuer 
automatiquement un texte sans accent avec un taux de succès de près de 95% (c'est-à-dire qu'on 
commettra une erreur d'accent à peu près à tous les vingt mots).

Tout porte à croire qu'on peut atteindre des résultats de beaucoup supérieurs grâce à l'utilisation 
de modèles de langue plus ou moins sophistiqués, qui seront en mesure de lever les ambiguïtés 
résultant de l'absence d'accents, en se basant sur des considérations d'ordre linguistique. En 
particulier, il semble que les modèles de langue dits probabilistes soient particulièrement bien 
adaptés pour ce genre de tâche, parce qu'ils fournissent un critère de désambiguïsation quantitatif : 
lorsqu'on rencontre une forme sans accent à laquelle peuvent correspondre plusieurs formes 
valides (portant ou non des accents), on choisit la plus probable, en se basant sur le contexte 
immédiat et sur un ensemble d'évènements observés antérieurement (le "corpus d'entraînement").

Notons que cette idée n'est pas entièrement originale : El-Bèze et al. exposent dans [3] une 
technique de réaccentuation qui s'inspire des mêmes concepts, alors que Yarowsky obtient des 
résultats comparables dans [6], en combinant différents critères de désambiguïsation statistiques 
dans un cadre unificateur (les listes de décision).

2	Réaccentuation automatique

Nous avons mis au point un programme de réaccentuation automatique, que nous appelons 
Reacc, basé sur un modèle de langue stochastique. Reacc accepte en entrée une chaîne de 
caractères représentant un texte français sans accent. Si la chaîne d'entrée contient des accents, 
on peut bien sûr la désaccentuer : comme à tout caractère accentué ne correspond qu'un seul 
caractère sans accent, ce processus est entièrement déterministe. Une autre possibilité est de 
conserver les accents, en prenant pour acquis qu'ils sont corrects. Dans un cas comme dans 
l'autre, la sortie attendue de Reacc est une chaîne de caractères qui ne diffère de la chaîne d'entrée 
que par les accents : on s'attend à recevoir en sortie le même texte français, mais correctement 
accentué.

Reacc procède donc en trois étapes successives : segmentation, génération d'hypothèses et 
désambiguïsation.

L'unité sur laquelle opère Reacc est le mot. L'exercice de segmentation consiste donc à prendre la 
chaîne d'entrée et à y localiser les frontières entre les mots, incluant les signes de ponctuation, de 
même que les nombres et autres expressions combinant chiffres et lettres. La segmentation repose 
sur un ensemble de règles décrivant des connaissances générales sur la structure des textes 
électroniques. Très peu de ces connaissances sont spécifiques au français. On retrouve quand 
même une liste d'abréviations et acronymes courants, qui sert à déterminer si un point accolé à une 
suite de caractères alphabétiques appartient à ce mot, ou agit comme point final. On utilise aussi 
une liste des constructions les plus fréquentes impliquant le tiret et l'apostrophe en français, afin de 
déterminer s'ils agissent ou non comme frontière de mots : l'école versus aujourd'hui, passe-
montagne versus pensez-vous.

L'étape suivante, la génération d'hypothèses, consiste à produire, pour chaque mot identifié lors de 
la segmentation, la liste de toutes les possibilités d'accentuation. Par exemple, si on a isolé l'unité 
cote, on veut générer les formes cote, coté, côte, côté. En fait, rien n'empêche qu'on génère aussi 
les formes côtè, cötê, etc. En pratique, toutefois, il importe de limiter autant que possible le nombre 
d'hypothèses, de façon à réduire le potentiel d'explosions combinatoires. On a donc recours à une 
liste de toutes les formes françaises valides, formes fléchies incluses, indexées sur leurs versions 
désaccentuées. En théorie, une telle liste peut contenir plusieurs centaines de milliers de formes 
distinctes. En pratique, on peut couper ce nombre de moitié, en excluant les formes qui ne portent 
pas d'accents et pour lesquelles il n'existe pas de variante accentuée valide. On peut réaliser des 
économies supplémentaires en excluant les formes les moins fréquentes, mais dans ce cas, il faut 
s'attendre à une baisse de la performance.

Une fois les hypothèses générées, il faut choisir les plus vraisemblables : c'est ce qu'on appelle la 
désambiguïsation. Nous utilisons pour ce faire un modèle de langue stochastique, appelé modèle 
de Markov caché (l'implantation que nous utilisons est le package lm de Foster [4]). Dans ce 
modèle, un texte est vu comme le résutat de deux processus stochastiques distincts. Le premier 
processus génère une suite de symboles qui, dans notre modèle, correspondent à des étiquettes 
morpho-syntaxiques (par exemple : NomCommun-masculin-singulier, Verbe-Indicatif-présent-
3ième-personne-pluriel). Dans un modèle markovien d'ordre N, la production d'un symbole dépend 
uniquement des N-1 symboles précédents. La séquence d'étiquettes produite constitue le 
phénomène caché d'où le modèle tire son nom. Le deuxième processus génère alors, pour chaque 
étiquette de la séquence, un autre symbole qui, cette fois-ci, correspond à une forme (un mot) du 
langage. Cette deuxième séquence est le résultat observable.

Les paramètres de notre modèle sont donc :

·	P(ti | hi-1) : La probabilité d'observer une étiquette ti, étant données les N-1 étiquettes précé-
dentes (hi-1 désigne la suite d'étiquettes de longueur N-1 se terminant à la position i-1).

·	P(fi | ti) : La probabilité d'observer une forme fi, étant donnée l'étiquette sous-jacente ti.

Bien entendu, la valeur exacte de ces paramètres est inconnue, mais en pratique, on peut en faire 
l'estimation à partir de fréquences observées dans un corpus d'entraînement. Ce corpus consiste 
en un ensemble de phrases, à chaque mot duquel est acollée l'étiquette appropriée (en d'autres 
mots : un corpus dans lequel la nature du phénomène caché nous est "révélée"). La taille du corpus 
doit être suffisante pour assurer une estimation fiable de la valeur de chaque paramètre. À défaut 
d'un tel corpus étiquetté, on peut effectuer l'entraînement à partir d''un texte non-étiquetté, pour 
ensuite raffiner la valeur des paramètres par réestimation. On peut aussi combiner ces deux 
méthodes, c'est-à-dire obtenir une première estimation des paramètres à partir d'un petit corpus 
étiquetté, pour ensuite en faire la réestimation sur la base d'un corpus non-étiquetté de plus grande 
taille.

Étant donnés ces paramètres, on peut évaluer la probabilité globale d'une suite de mots 
s = s1s2...sn. Soit T, l'ensemble de toutes les séquences d'étiquettes de longueur n possibles :
Bien que le calcul direct de cette équation requière un nombre d'opérations exponentiel en n, il 
existe un algorithme qui produit le même résultat en temps polynomial (voir [5]).

Notre stratégie de désambiguïsation consiste à choisir la suite d'hypothèses qui produit la version 
du texte dont la probabilité globale est maximale. En d'autres mots, si on représente le texte et ses 
hypothèses d'accentuation comme un graphe acyclique dirigé (DAG), le problème peut se formuler 
comme la recherche du chemin, allant du début à la fin du texte, dont la probabilité est maximale 
(figure 1).



Figure 1: Représentation d'un texte et des hypothèses d'accentuation sous forme de 
graphe acyclique dirigé

Le calcul de ce chemin pose bien entendu des problèmes de complexité de calcul, puisque le 
nombre de chemins à explorer croît en général de façon exponentielle avec la longueur du texte. 
En pratique, toutefois, il est possible de segmenter le graphe en îlots indépendants, c'est-à-dire en 
sections pour lesquelles le chemin optimal est indépendant du reste du graphe. Typiquement, on 
considère que les phrases sont indépendantes les unes des autres. On peut donc segmenter le 
texte en phrases et calculer le chemin optimal pour chaque phrase. Si le nombre de possibilités à 
l'intérieur d'une phrase demeure problématique, il existe des moyens de resegmenter celle-ci, au 
prix d'une légère dégradation de la précision. Dans notre implantation, chaque phrase est 
découpée en segments tels que le nombre de chemins à explorer à l'intérieur d'un segment 
n'excède pas un certain seuil (que nous appelons le paramètre S). Les points de coupe sont choisis 
au moyen d'une heuristique simple qui tend à minimiser la dépendance entre les segments : dans 
la mesure du possible, chaque segment doit se terminer par une suite de mots non-ambigus, c'est-
à-dire pour lesquels il n'existe à la fois qu'une seule hypothèse d'accentuation et une seule analyse 
lexicale. On traite alors successivement les segments de gauche à droite, et on préfixe chaque 
segment avec les derniers mots du chemin optimal du segment précédent.

Une fois la désambiguïsation effectuée, il reste à produire un résultat. Cette opération est en réalité 
très simple, mais quand même digne d'intérêt. En effet, un de nos principaux soucis est de 
préserver dans la sortie l'apparence du texte d'entrée. Il faut donc partir de chaque forme 
apparaissant sur le chemin optimal du graphe, retrouver la forme correspondante dans la chaîne 
d'entrée, et transposer l'accentuation de la nouvelle forme sur la forme originale, sans autrement 
en modifier l'apparence.

3	Évaluation

Pour évaluer la performance d'une méthode de réaccentuation, il suffit de choisir un texte ou un 
ensemble de textes français correctement accentués, de les désaccentuer automatiquement, de 
soumettre le tout au programme de réaccentuation, et de comparer les résultats obtenus au texte 
original.

Une des propriétés de Reacc que nous souhaitions évaluer était sa capacité à fonctionner avec des 
textes de nature variée. Pour ce faire, l'idéal aurait été de soumettre à notre programme un corpus 
"balancé", du même genre que le Brown Corpus. Comme nous ne disposions pas d'une telle 
ressource pour le français, nous avons dû confectionner notre propre corpus, à partir de documents 
qui nous étaient disponibles.

Le corpus de test est donc constitué d'extraits de textes français accentués, provenant de sept 
sources différentes, représentées à peu près également : on y retrouve des textes du domaine 
militaire, des textes juridiques, des publications des Nations Unies, des textes littéraires, des 
revues de presse, des manuels informatiques et des extraits du Hansard canadien (journal des 
débats à la Chambre de Communes). L'ensemble totalise 57 966 mots (ce compte a été produit au 
moyen de l'utilitaire UNIX wc). Certaines modifications ont été apportées au texte, afin de corriger 
les quelques erreurs d'accentuation que nous avons pu déceler au fil des expériences.

Le générateur d'hypothèses de Reacc utilisait, pour nos tests, une liste de formes extraite du DMF, 
un dictionnaire morpho-syntaxique contenant près de 380 000 formes distinctes ([1]). En fait, ce 
nombre est probablement excessif. Nous avons d'ailleurs obtenu des résultats tout-à-fait 
satisfaisants lors d'expériences préliminaires, avec un dictionnaire ne reconnaissant que 50 000 
formes environ.

Pour le modèle de langue, après différentes expériences, nous avons opté pour une approche qui 
privilégie la qualité des données sur leur quantité. Nous avons utilisé un modèle de Markov caché 
d'ordre 2, basé sur un ensemble d'environ 350 étiquettes morpho-syntaxiques. Les paramètres du 
modèle ont d'abord été initialisés à l'aide du DMF, c'est-à-dire qu'on a restreint d'emblée les P(fi | ti) 
en fonction du contenu des valeurs sanctionnées par le dictionnaire. On a ensuite procédé à un 
entraînement du modèle sur un corpus de texte de 60 000 mots, extrait du Hansard canadien, 
étiqueté à la main ([2]). On a finalement utilisé un corpus de texte beaucoup plus volumineux (plus 
de 3 millions de mots), non-étiqueté, afin de réestimer les paramètres du modèle.

Outre le générateur d'hypothèses et le modèle de langue utilisés, plusieurs paramètres affectent la 
performance de Reacc, tant sur le plan de la qualité des résultats obtenus que sur celui du temps-
machine. Néanmoins, le facteur le plus important est le paramètre S, qui limite la taille des 
segments sur lesquels Reacc travaille. On retrouve dans la tableau 1 les résultats obtenus pour 
différentes valeurs de S (une augmentation exponentielle de ce facteur se traduit en général par 
une augmentation linéaire de la longueur des segments traités). Les tests ont été effectués sur une 
machine Sun SPARCstation 10.

    Nombre maximum 
    d'hypothèses par 
    segment (S)

    Temps-machine 
    (secondes)

    Nombre total 
    d'erreurs
    (mots)

    Distance moyenne 
    entre les erreurs 
    (mots)

    Tableau 1: Résultats des réaccentuations

Un examen sommaire des résultats obtenus révèle qu'on a fort à gagner en permettant au système 
de travailler sur des segments plus longs. Toutefois, passée une certaine limite, la qualité des 
résultats tend à plafonner, alors que les temps d'exécution, eux, grimpent en flèche. Tout 
dépendant du genre d'application et des ressources disponibles, il semblerait qu'on puisse compter 
sur des résultats acceptables dès lors que S est fixé à l'entour de 16 ou 32.

Il est intéressant d'examiner où Reacc se trompe. On retrouve dans le tableau 2 une classification 
grossière des erreurs de réaccentuation que Reacc a commises sur notre corpus de test, lorsque 
S était fixé à 16. La catégorie qui arrive en tête regroupe assez libéralement les erreurs qui ont pour 
point en commun qu'elles résultent d'un mauvais choix sur la présence d'un accent aigu sur le e de 
la syllabe finale (par exemple : aime versus aimé). Viennent ensuite les erreurs qui découlent de 
"lacunes" du générateur d'hypothèses, c'est-à-dire de cas où celui-ci ne connaît tout simplement 
pas la forme correctement accentuée. Dans la majorité des cas, il s'agit de noms propres (près de 
la moitié, en fait), mais on rencontre aussi, surtout dans les textes de nature plus technique, 
beaucoup d'abréviations, de mots non-français et de "néologismes" (par exemple : 
réaménagement, séropositivité). La catégorie qui vient ensuite concerne une unique paire de 
mots : la préposition à et la forme a du verbe avoir.

    Type d'erreur

    Nombre

    Pourcentage

    Ambiguïtés -e / -é

    Formes inconnues

    Ambiguïté a / à

    Autres

    Total

    Tableau 2: Classification des erreurs d'accentuation
    (S = 16)

4	Conclusions

Nous avons présenté une méthode de réaccentuation automatique des textes français, basée sur 
un modèle de langue markovien caché. Cette méthode a fait l'objet d'une implantation réelle : le 
programme Reacc. Nos expériences ont démontré que ce programme produisait des textes d'une 
qualité tout-à-fait acceptable, dans des temps plus que raisonnables : on peut atteindre une 
moyenne d'une erreur d'accentuation aux 130 mots, en traitant plus de 20 000 mots à la minute.

Bien entendu, il y a toujours place à des améliorations. En particulier, il est certain que l'utilisation 
d'un modèle de langue plus fin (par exemple, un modèle d'ordre 3) ne pourrait qu'améliorer la 
qualité de la désambiguïsation. Compte tenu aussi de la forte proportion d'erreurs d'accentuation 
causées par des lacunes au dictionnaire, il serait intéressant d'examiner des façons de traiter ces 
"mots inconnus". À cet égard, nous avons déjà effectué certaines expériences préliminaires, qui ont 
produit des résultats particulièrement intéressant. En particulier, nous nous sommes intéressés à 
des façons de "deviner" l'accentuation d'un mot inconnu, à partir d'une modélisation stochastique 
de l'accentuation des mots connus. Il reste toutefois beaucoup de travail à faire de ce côté.

Par ailleurs, les méthodes que nous avons exposées ouvrent la porte à d'autres applications du 
même genre. Par exemple, on peut voir comment les méthodes de réaccentuation pourraient être 
généralisées, afin de traiter d'autres types de pertes d'information. On pense tout particulièrement 
aux textes dont tous les caractères accentués ont été remplacés par un caractère unique 
(typiquement, un point d'interrogation), ou aux textes dont le huitième bit de chaque caractère a été 
perdu. Dans de tels textes le é apparaît comme un i, le è comme un h, etc. Dans ces cas, au 
problème de l'ambiguïté lexicale s'ajoute celui du découpage, qui devient lui aussi ambigu.

Une autre possibilité intéressante est de greffer un programme du genre de Reacc à un logiciel de 
traitement de texte, d'une manière telle que l'utilisateur puisse taper un texte français sans se 
soucier des accents, qui sont alors insérés automatiquement à mesure que le texte est produit. De 
la réaccentuation, on passe ainisi à l'accentuation automatique. Un tel mécanisme pourrait faciliter 
sensiblement la saisie des textes français. (On sait combien les conventions de saisie des accents 
au clavier sont variées et pas toujours très ergonomiques.)

Une application beaucoup plus ambitieuse, se basant sur des méthodes similaires, est la rédaction 
assistée par ordinateur. Dans ce cas, plutôt que de travailler sur le texte déjà tapé par l'utilisateur, 
l'ordinateur s'intéresse au texte à venir, et essaie de prévoir ce que l'utilisateur va taper, de façon 
à lui éviter la saisie de grandes portions de texte. 

Toutes ces applications font présentement l'objet de travaux de recherche au CITI.

Références

[1]	Bourbeau, Laurent et François Pinard, 1987, Dictionnaire Micro-informatisé du Français 
(DMF), Progiciels Bourbeau Pinard Inc., 1501 avenue Ducharme, Montréal, H2V 1G2.

[2]	Bourbeau, Laurent, 1994, Fabrication d'un corpus témoin bilingue étiqueté et annoté pour la 
mise au point de techniques de parsage automatique probabiliste, Rapport technique 
présenté par Progiciels Bourbeau Pinard, Centre d'innovation en technologies de l'information 
(CITI), Laval.

[3]	El-Bèze, Marc, Bernard Mérialdo, Bénédicte Rozeron et Anne-Marie Derouault, 1994, 
"Accentuation automatique de textes par des méthodes probabilistes", dans Technique et 
sciences informatiques, Vol 13, no 6, pp. 797 - 815.

[4]	Foster, George F., 1995, Communication personnelle.

[5]	Rabiner, L. R. et B. H. Juang, janvier 1986, "An Introduction to Hidden Markov Models", dans 
IEEE ASSP Magazine.

[6]	Yarowsky, David, 1994, "Decision Lists for Lexical Ambiguity Resolution: Application to Accent 
Restoration in Spanish and French", dans Proceeding of the 32nd Annual Meeting of the 
Association for Computational Linguistics (ACL-94), pp. 88-95.

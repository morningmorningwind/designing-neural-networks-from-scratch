<!DOCTYPE CESANA PUBLIC "-//CES//DTD cesAna//EN" >
<CESANA VERSION="1.12">
<CHUNKLIST>
<CHUNK>
<PAR>
<S ID="47">
Automatic Restoration of Accents in French Text
</S>
<S ID="62">
Michel Simard
</S>
<S ID="76" new>
simard@citi.
</S>
<S ID="80" new>
doc.
</S>
<S ID="82">
ca
</S>
<S ID="171" new>
Industry Canada Centre for Information Technology Innovation (CITI) 1575 Chomedey Blvd.
</S>
<S ID="200">
Laval, Quebec Canada H7V 2X2
</S>
<S ID="203">
1
</S>
<S ID="216">
Introduction
</S>
<S ID="354">
The research presented in this report is part of the Robustness Project conducted by the Computer- Aided Translation (CAT) team at CITI.
</S>
<S ID="459">
This project is intended to develop methods and tools for the  "robust" processing of natural languages:
</S>
<S ID="674">
many natural-language processing (NLP) systems either  refuse to process texts that contain errors or elements that are foreign to their particular knowledge base, or behave unpredictably under such circumstances.
</S>
<S ID="754">
In contrast, a "robust" system will behave  in a predictable and useful manner.
</S>
<S ID="893">
Unaccented French texts (i.e., texts without diacritics) are a typical and particularly common  example of problems faced by NLP systems.
</S>
<S ID="957" new>
Such problems are most often encountered in an  E-mail context.
</S>
<S ID="987">
Two factors account for this:
</S>
<S ID="1220">
First, the computer field has long suffered from a lack  of sufficiently widespread standards for encoding accented characters, which has resulted in a plethora of problems in the electronic transfer and processing of French texts.
</S>
<S ID="1383">
Moreover, it is not  uncommon for one of the software links in an E-mail distribution chain to deliberately remove accents in order to avoid subsequent problems.
</S>
<S ID="1494">
Secondly, keying in accented characters is still a  cumbersome activity, at times requiring manual acrobatics.
</S>
<S ID="1546">
This is a matter of both standards and  ergonomics.
</S>
<S ID="1678">
As a result, a large number of French-speaking users systematically avoid using  accented characters, at least when writing E-mail.
</S>
<S ID="1864">
If this situation remains tolerable in practice, it is essentially because it is extremely rare that the  absence of accents renders a French text incomprehensible to the human reader.
</S>
<S ID="2005">
From a linguistic  point of view, the lack of accents in French simply increases the relative degree of ambiguity inherent in the language.
</S>
<S ID="2119">
At worst, it slows down reading and proves awkward, much as a text  written entirely in capital letters might do.
</S>
<S ID="2311">
The fact remains, however, that while unaccented French text may be tolerated under certain  circumstances, it is not acceptable in common usage, especially in the case of printed documents.
</S>
<S ID="2390">
Furthermore, unaccented texts pose serious problems for automatic processing.
</S>
<S ID="2677">
Whether for  purposes of documentary research, spelling, grammar or style checkers, machine translation, natural-language interface, or any other form of language processing, accents are generally required in French texts-hence the interest in methods of automatic accent restoration.
</S>
<S ID="2944">
An examination of the problem reveals that the vast majority (approximately 85%) of the words in  French texts take no accents, and that the correct form of more than half of the remaining words can be deduced deterministically on the basis of the unaccented form.
</S>
<S ID="3152">
Consequently, with the use  of a good dictionary, accents can be restored to an unaccented text with a success rate of nearly 95% (i.e., an error in accentuation will occur in approximately every 20 words).
</S>
<S ID="3398">
All evidence suggests that much higher results can be attained through the use of moderately  sophisticated language models, which will be able to deal with ambiguities resulting from missing accents, on the basis of linguistic considerations.
</S>
<S ID="3593">
More specifically, it would seem that language  models termed probabilistic are particularly well adapted to this sort of task, as they provide a quantitative criterion for resolving ambiguity.
</S>
<S ID="3862">
When the system encounters an unaccented form that  could correspond to various valid word forms (that may or may not take accents), it chooses the most probable one on the basis of the immediate context and a set of events observed previously (the training corpus).
</S>
<S ID="3919">
Note, however, that this idea is not entirely original.
</S>
<S ID="4177">
El-Bèze et al. [3] describe an accent-restoration  technique that draws upon the same concepts, while Yarowsky has obtained comparable results [6] by combining different criteria for statistical disambiguation within a unifying framework (decision lists).
</S>
<S ID="4180">
2
</S>
<S ID="4209">
Automatic Accent Restoration
</S>
<S ID="4282" new>
We have developed an automatic accent restoration program called Reacc.
</S>
<S ID="4327">
It is based on a  stochastic language model.
</S>
<S ID="4418">
Reacc will accept as input a string of characters that represents  unaccented French text.
</S>
<S ID="4503">
If the input string contains accents, the accents can, of course, be stripped  away.
</S>
<S ID="4623">
Since each accented character can correspond to only one unaccented character, this  process is entirely deterministic.
</S>
<S ID="4706">
Another option is to retain the accents, on the assumption that they  are correct.
</S>
<S ID="4843">
In either case, the output expected from Reacc is a string of characters that differs from  the input string solely in terms of accents.
</S>
<S ID="4963">
The expected output is therefore the same French text,  the only difference being that the words are properly accented.
</S>
<S ID="5065">
Reacc performs three successive operations: segmentation, hypothesis generation and  disambiguation.
</S>
<S ID="5118">
The unit on which the program operates is the word.
</S>
<S ID="5291">
Therefore, the segmentation process consists  in taking the input string and locating word boundaries, including punctuation marks, numbers and number/letter combinations.
</S>
<S ID="5449">
Segmentation relies almost exclusively on language-independant  data, i.e. a set of rules encoding general knowledge about the structure of electronic texts.
</S>
<S ID="5691">
One  exception to this is a list of French abbreviations and current acronyms, which is used to determine whether a period following a string of alphabetic characters (i.e., a word) belongs to the string itself or serves to end a sentence.
</S>
<S ID="6180">
Also, a list of the most prevalent constructions involving the hyphen  and apostrophe in French is needed to determine whether or not these symbols act as word boundaries-for instance, compare l'école (in which case the apostrophe replaces an elided vowel in the article la and serves to link it with the noun that follows) with aujourd'hui (a single word with a compound origin), and passe-montagne (a compound noun) with pensez-vous (an interrogative inversion of pronoun and verb).
</S>
<S ID="6337">
The next step, hypothesis generation, consists in producing a list of all accentuation possibilities for  each word identified in the segmentation process.
</S>
<S ID="6449">
For example, if the unit cote has been isolated,  the system would have to generate cote, coté, côte, and côté.
</S>
<S ID="6537">
Note that nothing precludes the  generation of nonexistent words such as côtè and cötê.
</S>
<S ID="6698">
In practice, though, it is important to limit  the number of hypotheses as much as possible so as to reduce the possibility of an undue number of combinations.
</S>
<S ID="6835">
The system thus draws upon a list of all valid French forms, including inflections,  indexed according to their unaccented counterparts.
</S>
<S ID="6927">
In theory, such a list could contain several  hundreds of thousands of distinct word forms.
</S>
<S ID="7080">
In practice, though, the number can be reduced by  half, by excluding the forms that take no accents and for which there are no valid accented variants.
</S>
<S ID="7222">
The number can be further reduced by excluding the lower-frequency forms; however, this will  eventually result in a decline in performance.
</S>
<S ID="7309">
Once all the hypotheses have been generated, the most probable ones must be selected.
</S>
<S ID="7346">
This step  is called disambiguation.
</S>
<S ID="7498">
A stochastic language model, called a hidden Markov model (HMM) is  used (by means of Foster's Im package [4]) to carry out the disambiguation process,
</S>
<S ID="7591">
According to  this model, a text is seen as the result of two distinct stochastic processes.
</S>
<S ID="7771">
The first generates a  series of symbols which, in our model, correspond to morpho-syntactic tags (e.g., CommonNoun- masculine-singular; Verb-indicative-present-3rdPerson-plural).
</S>
<S ID="7868">
In an N-class HMM, the  production of a symbol depends exclusively on the preceding N-1 symbols.
</S>
<S ID="7978">
The sequence of tags  produced constitutes a hidden phenomenon (from which the name of the model is derived).
</S>
<S ID="8137">
Then  for each tag in the sequence, the second process generates another symbol-in this instance, one that corresponds to a word that exists in the language.
</S>
<S ID="8185">
This second sequence is the observable  result.
</S>
<S ID="8230">
The parameters of our model are as follows:
</S>
<S ID="8247">
·	P(ti | hi-1):
</S>
<S ID="8379">
The probability of observing tag ti, given the preceding N-1 tags (hi-1 designates the  series of N-1 tags ending at position i-1).
</S>
<S ID="8394">
·	P(fi | ti):
</S>
<S ID="8461">
The probability of observing form fi, given the underlying tag ti.
</S>
<S ID="8623">
The exact value of these parameters is, of course, unknown, but in practice, an estimate can be  made on the basis of frequencies observed in a training corpus.
</S>
<S ID="8804">
The corpus consists of a series of  sentences, each word of which is assigned an appropriate tag (i.e., a corpus within which the nature of the "hidden" phenomenon is "revealed").
</S>
<S ID="8899">
The corpus must be large enough to ensure a reliable  estimate of the value of each parameter.
</S>
<S ID="9059">
If no such tagged corpus is available, the training operation  can be carried out on an untagged text, and the parameters subsequently refined by reestimation.
</S>
<S ID="9108" new>
Another option is to combine the two methods-i.
</S>
<S ID="9279">
e., to obtain an initial estimate of the parameters  on the basis of a small tagged corpus and then proceed with a reestimation on the basis of a larger, untagged corpus.
</S>
<S ID="9363" new>
Given these parameters, the overall probability of a sequence of words s = s1s2...
</S>
<S ID="9384">
sn can be  evaluated.
</S>
<S ID="9449">
If T is the set of all possible n-length tagged sequences, then:
</S>
<S ID="9636">
Although the direct calculation of this equation requires a number of operations exponential in n,  there is an algorithm that will produce the same results in polynomial time (see [5]).
</S>
<S ID="9785">
Our disambiguation strategy consists in choosing a series of hypotheses that produce the version  of the text with the highest overall probability.
</S>
<S ID="10063">
In other words, if a given text segment and its  accentuation hypotheses are represented as a directed acyclic graph (DAG), the problem can be expressed as the search, from the beginning to the end of the text segment, for the pathway with the highest probability (figure 1).
</S>
<S ID="10076">
Figure 1:
</S>
<S ID="10187">
Representation of a text segment and possible accentuation hypotheses in  the form of a directed acyclic graph
</S>
<S ID="10403">
There are, of course, computational complexity problems involved in the calculation of this pathway,  as the number of pathways to be explored generally increases exponentially with the length of the text segment.
</S>
<S ID="10488" new>
In practice, however, it is possible to segment the graph into independent islets-i.
</S>
<S ID="10579">
e.,  into sections for which the optimal pathway is independent from the rest of the graph.
</S>
<S ID="10650">
Typically,  sentences are considered to be independent of one another.
</S>
<S ID="10749">
The text can thus be segmented into  sentences and the optimal pathway for each can be calculated.
</S>
<S ID="10918">
If the number of possibilities within  a given sentence remains problematic, there are ways of resegmenting the sentence, at the expense of a slight loss in accuracy.
</S>
<S ID="11110">
In our way of proceeding, each sentence is segmented such  that the number of pathways to be explored within a given segment does not exceed a certain threshold (referred to as parameter S).
</S>
<S ID="11225">
The segmentation points are chosen by means of a simple  heuristic that tends to minimize segment interdependence.
</S>
<S ID="11309" new>
As much as possible, each segment  must end with a series of non-ambiguous words-i.
</S>
<S ID="11397">
e., words for which there is only one  accentuation hypothesis and one lexical analysis.
</S>
<S ID="11549">
The segments are processed successively from  left to right, and each is prefixed with the last words of the optimal pathway of the preceding segment.
</S>
<S ID="11632">
Once the disambiguation process has been completed, the results must be produced.
</S>
<S ID="11705">
Though this  operation does merit attention, it is actually very simple.
</S>
<S ID="11816">
One of our primary concerns is to preserve  the appearance of the input text once it reaches the output stage.
</S>
<S ID="12101">
Therefore, we must start with  each word form that appears on the optimal pathway represented on the graph, find the corresponding ocurrence in the input string, and transpose the accentuation of the new form onto the original form without modifying its appearance in any other way.
</S>
<S ID="12104">
3
</S>
<S ID="12115">
Assessment
</S>
<S ID="12407">
In order to assess the performance of an accent-restoration method, one simply has to select a  French text or set of texts that are correctly accented, automatically strip them of accents, feed them into the accent-restoration program, and then compare the results with the original text.
</S>
<S ID="12521">
One of the properties of Reacc that we wanted to evaluate was its ability to operate on various types  of texts.
</S>
<S ID="12647">
In order to do so, the ideal would have been to run the program on a "balanced" corpus,  along the lines of the Brown corpus.
</S>
<S ID="12776">
However, since no such resource was available in French, we  had to construct our own corpus from the documents at our disposal.
</S>
<S ID="12943">
The training corpus was therefore composed of excerpts from accented French texts drawn from  seven different sources, represented in more or less equal proportions.
</S>
<S ID="13168">
The texts include military  documents, legal texts, United Nations publications, literary texts, press reviews, computer manuals, and excerpts from Hansard (the official record of proceedings in Canada's House of Commons).
</S>
<S ID="13243">
The corpus totals 57,966 words (a figure produced by the UNIX wc utility).
</S>
<S ID="13369">
Certain  adjustments were made to the texts in order to correct a few errors in accentuation that were found during testing.
</S>
<S ID="13609">
For the purposes of our tests, the Reacc hypothesis generator used a list of word forms taken from  the Dictionnaire micro-informatisé du français (DMF), a morpho-syntactic dictionary that contains nearly 380,000 distinct word forms [1].
</S>
<S ID="13663">
Such a large number of terms is probably unnecessary.
</S>
<S ID="13808">
In  fact, as fully satisfactory results were obtained during preliminary trials using a dictionary that recognized some 50,000 word forms only.
</S>
<S ID="13977">
Where the language model was concerned, after various trials, we opted for an approach that  placed a greater priority on the quality rather than the quantity of data.
</S>
<S ID="14060">
We used a bi-class HMM  based on a set of approximately 350 morpho-syntactic tags.
</S>
<S ID="14250">
The parameters of the model were first  initialized by means of the DMF-in other words, the P(fi | ti) were restricted according to the contents of the values sanctioned by the dictionary.
</S>
<S ID="14377">
We then went on to an initial training phase,  using a 60,000-word manually tagged corpus taken from the Canadian Hansard [2].
</S>
<S ID="14510">
Lastly, a much  larger untagged corpus was used, consisting of over 3 million words, in order to reestimate the model's parameters.
</S>
<S ID="14709">
Aside from the hypothesis generator and the language model used, a number of other parameters  affect Reacc's performance on the level of both the quality of the results obtained and running time.
</S>
<S ID="14831">
Nevertheless, the most important factor is the S parameter, which limits the size of the segments  that Reacc processes.
</S>
<S ID="15020">
Table 1 provides the results obtained for different S values (an exponential  increase in this factor generally translates into a linear increase in the length of the segments processed).
</S>
<S ID="15073">
The tests were carried out on a Sun SPARCstation 10.
</S>
<S ID="15090" new>
Maximum no.
</S>
<S ID="15130">
of      hypotheses per     segment (S)
</S>
<S ID="15163">
Running time      (seconds)
</S>
<S ID="15204">
Total number of      errors (words)
</S>
<S ID="15259">
Average distance      between errors     (words)
</S>
<S ID="15273">
Table 1:
</S>
<S ID="15310">
Results of Accent-Restoration Trials
</S>
<S ID="15433">
A cursory look at the results reveals that there is much to be gained by allowing the system to work  on longer segments.
</S>
<S ID="15558">
However, beyond a certain limit, the quality of the results tends to level off,  while the running time increases radically.
</S>
<S ID="15713">
Depending on the type of application and the resources  available, it would seem that acceptable results can be obtained when S is set at around 16 or 32.
</S>
<S ID="15767">
It is interesting to look at where Reacc goes wrong.
</S>
<S ID="15896">
Table 2 provides a rough classification of  accent-restoration errors made by Reacc on our training corpus when S was set at 16.
</S>
<S ID="16193">
The category  in which the greatest number of accentuation errors were made includes a rather liberal grouping of errors that have a common feature: they are the result of an incorrect choice pertaining to an acute accent on an e in the final syllable of a word (e.g., aime as opposed to aimé).
</S>
<S ID="16288" new>
The next group  of errors are those that stem from inadequacies in the hypothesis generator-i.
</S>
<S ID="16369">
e., cases in which  the generator simply does not know the correct accented form.
</S>
<S ID="16575">
In most cases (nearly half), proper  nouns are involved, but, especially in more technical texts, there are also many abbreviations, non- French words, and neologisms (e.g., réaménagement, séropositivité).
</S>
<S ID="16699">
The next category concerns  a unique word pair: the preposition à, and a, the third person singular form of the verb avoir.
</S>
<S ID="16718">
Type of error
</S>
<S ID="16730">
Number
</S>
<S ID="16746">
Percentage
</S>
<S ID="16774">
Ambiguities: -e vs. -é
</S>
<S ID="16798">
Unknown word forms
</S>
<S ID="16822">
Ambiguity: a vs. à
</S>
<S ID="16833">
Other
</S>
<S ID="16844">
Total
</S>
<S ID="16858">
Table 2:
</S>
<S ID="16915">
Classification of Accent Restoration Errors     (S = 16)
</S>
<S ID="16918">
4
</S>
<S ID="16930">
Conclusions
</S>
<S ID="17058">
This report has presented a method of automatic accent restoration for French texts, based on a  hidden Markov language model.
</S>
<S ID="17128">
This method was actually implemented by means of the Reacc  software.
</S>
<S ID="17267">
Our experiments have demonstrated that this program produces texts that are altogether  acceptable within a totally reasonable time frame:
</S>
<S ID="17375">
we can expect an error to occur every 130 words  on average, and the processing of 20,000 words per minute.
</S>
<S ID="17426">
There is, of course, always room for improvement.
</S>
<S ID="17569">
In particular, the use of a more refined language  model (e.g., a tri-class HMM) could only enhance the quality of the disambiguation process.
</S>
<S ID="17760">
Moreover, given the large proportion of accentuation errors caused by words not recognized by the  dictionary, it would be interesting to examine means of dealing with such "unknown" words.
</S>
<S ID="17890">
In this  regard, we have already carried out certain preliminary experiments which have produced especially interesting results.
</S>
<S ID="18051">
In particular, we have focused on ways of "guessing" the accentuation  of an unknown word on the basis of a stochastic model of the accentuation of known words.
</S>
<S ID="18120">
There  is nevertheless a great deal of work to be done in this area.
</S>
<S ID="18197">
The methods we described here open the door to other, similar applications.
</S>
<S ID="18533">
For example, we can  see how generalizations could be drawn from accent-restoration methods in order to deal with other types of information loss-especially texts in which all accented characters have been replaced by a single character (most often a question mark), or texts in which the eighth bit of each character has been lost.
</S>
<S ID="18597">
For instance, in such texts, é comes out as an i and è as an h.
</S>
<S ID="18751">
In such cases, a  problem of determining word boundaries compounds that of lexical ambiguity; word boundaries thus become a source of ambiguity as well.
</S>
<S ID="19004">
Another interesting possibility is that of grafting a program such as Reacc onto a word processing  application so that the user can input a French text without worrying about accents, which would be inserted automatically as the text is being input.
</S>
<S ID="19085">
This would thus mark a shift from accent restoration  to automatic accentuation.
</S>
<S ID="19304">
This type of feature could significantly facilitate the inputting of French  texts, especially in light of the lack of uniformity and ergonomic soundness in the conventions for producing accents on computer keyboards.
</S>
<S ID="19410">
A much more ambitious application that could derive from similar methods is computer-assisted  writing.
</S>
<S ID="19646">
Instead of processing a text already input by the user, the computer would be concerned  with text yet to be formulated and would try to foresee what the user will type so as to avoid the need to manually input large portions of text.
</S>
<S ID="19710">
All of these applications are currently being studied at CITI.
</S>
<S ID="19722">
References
</S>
<S ID="19727">
[1]
</S>
<S ID="19767" new>
Bourbeau, Laurent, and François Pinard.
</S>
<S ID="19773" new>
1987.
</S>
<S ID="19824" new>
Dictionnaire micro-informatisé du français  (DMF).
</S>
<S ID="19866">
Montreal: Progiciels Bourbeau Pinard inc.
</S>
<S ID="19871">
[2]
</S>
<S ID="19890" new>
Bourbeau, Laurent.
</S>
<S ID="19896" new>
1994.
</S>
<S ID="20029" new>
Fabrication d'un corpus témoin bilingue étiqueté et annoté pour la  mise au point de techniques de parsage automatique probabiliste.
</S>
<S ID="20155">
Technical report submitted  by Progiciels Bourbeau Pinard to the Centre for Information Technology Innovation (CITI), Laval.
</S>
<S ID="20160">
[3]
</S>
<S ID="20237" new>
El-Bèze, Marc, Bernard Mérialdo, Bénédicte Rozeron and Anne-Marie Derouault.
</S>
<S ID="20246" new>
1994.  "
</S>
<S ID="20313" new>
Accentuation automatique de textes par des méthodes probabilistes."
</S>
<S ID="20367" new>
In Technique et  sciences informatiques, vol. 13, no.
</S>
<S ID="20383">
6, pp. 797-815.
</S>
<S ID="20388">
[4]
</S>
<S ID="20412" new>
Foster, George F. 1995.
</S>
<S ID="20436">
Personal communication.
</S>
<S ID="20441">
[5]
</S>
<S ID="20474" new>
Rabiner, L. R., and B. H. Juang.
</S>
<S ID="20482" new>
1986. "
</S>
<S ID="20523" new>
An Introduction to Hidden Markov Models."
</S>
<S ID="20562">
In IEEE  ASSP Magazine, January issue.
</S>
<S ID="20567">
[6]
</S>
<S ID="20584" new>
Yarowsky, David.
</S>
<S ID="20592" new>
1994. "
</S>
<S ID="20699" new>
Decision Lists for Lexical Ambiguity Resolution: Application to Accent  Restoration in Spanish and French."
</S>
<S ID="20812">
In Proceedings of the 32nd Annual Meeting of the  Association for Computational Linguistics (ACL-94), pp. 88-95.
</S>
</PAR>
</CHUNK>
</CHUNKLIST>
</CESANA>

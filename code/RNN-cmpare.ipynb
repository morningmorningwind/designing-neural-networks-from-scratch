{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入模块\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "from torch import nn\n",
    "\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个简单RNN层\n",
    "class ERNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(ERNN,self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.ih_linear = nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.hh_linear = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        \n",
    "    def init_h(self, x):\n",
    "        self.ht = torch.randn_like(x[0])\n",
    "    \n",
    "    def forward(self, x, h=None):\n",
    "        if h is None:\n",
    "            self.init_h(x)\n",
    "        seq_length, batch_size, input_size = x.size()\n",
    "        y = []\n",
    "        for t in range(seq_length):\n",
    "            self.ht = torch.tanh(self.ih_linear(x[t]) + self.hh_linear(self.ht))\n",
    "            y.append(self.ht.unsqueeze(0))\n",
    "        y = torch.cat(y)\n",
    "        return y, self.ht\n",
    "\n",
    "# 定义一个LSTM层\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(LSTM,self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.ii_linear = nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.hi_linear = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.if_linear = nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.hf_linear = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.ig_linear = nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.hg_linear = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.io_linear = nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.ho_linear = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "    def init_h(self, x):\n",
    "        self.ht = torch.randn_like(x[0])\n",
    "    def init_c(self, x):\n",
    "        self.ct = torch.randn_like(x[0])\n",
    "    \n",
    "    def forward(self, x, h=None, c=None):\n",
    "        if h is None:\n",
    "            self.init_h(x)\n",
    "        if c is None:\n",
    "            self.init_c(x)\n",
    "        seq_length, batch_size, input_size = x.size()\n",
    "        y = []\n",
    "        for t in range(seq_length):\n",
    "            it = torch.sigmoid(self.ii_linear(x[t]) + self.hi_linear(self.ht))\n",
    "            ft = torch.sigmoid(self.if_linear(x[t]) + self.hf_linear(self.ht))\n",
    "            gt = torch.tanh(self.ig_linear(x[t]) + self.hg_linear(self.ht))\n",
    "            ot = torch.sigmoid(self.io_linear(x[t]) + self.ho_linear(self.ht))\n",
    "            self.ct = ft * self.ct + it * gt\n",
    "            self.ht = ot * torch.tanh(self.ct)\n",
    "            y.append(self.ht.unsqueeze(0))\n",
    "        y = torch.cat(y)\n",
    "        return y, self.ht\n",
    "\n",
    "    \n",
    "# 定义一个GRU层\n",
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(GRU,self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.in_linear = nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.hn_linear = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.ir_linear = nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.hr_linear = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.iz_linear = nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.hz_linear = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "    def init_h(self, x):\n",
    "        self.ht = torch.randn_like(x[0])\n",
    "    \n",
    "    def forward(self, x, h=None):\n",
    "        if h is None:\n",
    "            self.init_h(x)\n",
    "        seq_length, batch_size, input_size = x.size()\n",
    "        y = []\n",
    "        for t in range(seq_length):\n",
    "            rt = torch.sigmoid(self.ir_linear(x[t]) + self.hr_linear(self.ht))\n",
    "            zt = torch.sigmoid(self.iz_linear(x[t]) + self.hz_linear(self.ht))\n",
    "            nt = torch.tanh(self.in_linear(x[t]) + rt * self.hn_linear(self.ht))\n",
    "            self.ht = (1 - zt) * nt + zt * self.ht\n",
    "            y.append(self.ht.unsqueeze(0))\n",
    "        y = torch.cat(y)\n",
    "        return y, self.ht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data summary:\n",
      "\n",
      " number of poems: 42\n",
      " number of words: 773\n",
      "\n",
      "Poem examples:\n",
      "\n",
      "S平生何所寄？天地一孤篷。郁纡且行游，迟复尘景中。E\n",
      "S星汉奔岩屿，惊涛卷曈虹。翕趿隐烟色，长桥海岛空。E\n",
      "S百年如云梦，逆旅何匆匆。吟坐忘知闻，拈花鉴溟濛。E\n",
      "S道心不外求，日影养虚冲。观风遣剑意，抱朴任穷通。E\n",
      "S千古一杯清，卧剑亦何如？云雁有芳信，谈笑未成书。E\n",
      "S故国弛山色，春华因才逐。北庭惜玉折，积风待岁除。E\n",
      "S俯仰苍茫间，太虚应有诸。值此吟月夜，借居怀纡余。E\n",
      "S心斋即坛醮，守道安违俗。江湖得意气，狂歌岂踟躇。E\n",
      "S所忧非尘辙，萧萧演六虚。冷眼任霜雪，平生性慵疏。E\n",
      "S浮景或可悲，愁予感韫椟。明日放归去，长梦酬三馀。E\n"
     ]
    }
   ],
   "source": [
    "# 训练一个基于ERNN神经网络来作诗\n",
    "\n",
    "## 读入用GloVe处理得到的文字 embeddings，以及句子数据。\n",
    "import codecs\n",
    "\n",
    "with codecs.open('data/word_embeddings_manyun_128.txt', mode='r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "n_words = len(lines) + 1\n",
    "word_emb_dim = input_size = 128\n",
    "word_embeddings = torch.nn.Embedding(n_words, word_emb_dim)\n",
    "i2w = {0:''}\n",
    "w2i = {'':0}\n",
    "for i in range(0, n_words - 1):\n",
    "    line = lines[i].split(' ')\n",
    "    i2w[i + 1] = line[0]\n",
    "    w2i[line[0]] = i + 1\n",
    "#    word_embeddings.weight[i] = torch.from_numpy(np.array(line[1:],dtype=np.float32))\n",
    "\n",
    "word_embeddings.weight.require_grad = False\n",
    "\n",
    "poems = []\n",
    "max_line_length = 32\n",
    "with codecs.open('data/manyun.txt', mode='r', encoding='utf-8') as f:\n",
    "    for poem in f:\n",
    "        poem = poem.replace(' ','')\n",
    "        if ':' in poem: poem = poem.split(':')[-1]\n",
    "        poem = poem.replace('\\n','')\n",
    "        poem = poem.replace('\\r','')\n",
    "        if len(poem) < 24 or len(poem) > max_line_length or '(' in poem or u'（' in poem or u'）' in poem or ')' in poem:\n",
    "            continue\n",
    "        poem = 'S' + poem + 'E'\n",
    "        poems.append(map(w2i.get, poem))\n",
    "\n",
    "n_poems = len(poems)\n",
    "\n",
    "print( 'Data summary:\\n\\n number of poems: {}\\n number of words: {}\\n'.format(n_poems, n_words))\n",
    "print('Poem examples:\\n\\n'+'\\n'.join([''.join(map(i2w.get, x)) for x in poems[:10]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S所忧非尘辙，萧萧演六虚。冷眼任霜雪，平生性慵疏。\n",
      "所忧非尘辙，萧萧演六虚。冷眼任霜雪，平生性慵疏。E\n"
     ]
    }
   ],
   "source": [
    "# 定义一个函数，随机返回一个 mini batch，用于训练，由于每一首诗歌的长度不同，我们此处规定每个batch只有一首诗。这样，就可以生成长度可变的诗歌。\n",
    "def get_batch(batch_size=1):\n",
    "    batch_raw = [poems[i][:] for i in np.random.randint(0, n_poems, batch_size)]\n",
    "    max_length = max(map(len, batch_raw))\n",
    "    for i in range(len(batch_raw)):\n",
    "        for j in range(len(batch_raw[i]),max_length):\n",
    "            batch_raw[i].append(w2i[''])\n",
    "    batch_raw = torch.LongTensor(batch_raw).detach().unsqueeze(2).transpose(0,1)\n",
    "    x = batch_raw[:-1].type(torch.float32)\n",
    "    y = batch_raw[1:]\n",
    "    return x, y\n",
    "\n",
    "def idx2emb(x):\n",
    "    return word_embeddings(x.type(torch.long)).squeeze(2).detach()\n",
    "    \n",
    "\n",
    "# 定义一个函数，输入一个 batch 返回句子\n",
    "def batch2sent(batch):\n",
    "    S = []\n",
    "    batch = batch.type(torch.int32).detach()\n",
    "    seq_length, batch_size, emb_size = batch.size()\n",
    "    for i in range(batch_size):\n",
    "        S.append(''.join(map(i2w.get, batch[:,i,:].view(-1).tolist())))\n",
    "    return u'\\n'.join(S)\n",
    "\n",
    "x, y = get_batch(1)\n",
    "print(batch2sent(x))\n",
    "print(batch2sent(y))\n",
    "\n",
    "# 定义一个生成器\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = LSTM(self.input_size, self.hidden_size)\n",
    "        self.output = nn.Linear(self.hidden_size, self.output_size)\n",
    "        self.logsoftmax = torch.nn.LogSoftmax(dim=-1)\n",
    "    def forward(self, x, h0=None):\n",
    "        seq_length, batch_size, input_size = x.size()\n",
    "        y, ht = self.rnn(x, h0)\n",
    "        y = y.view(-1, self.hidden_size)\n",
    "        y = self.output(y)\n",
    "        y = y.view(seq_length, batch_size, output_size)\n",
    "        y = self.logsoftmax(y)\n",
    "        return y, ht\n",
    "\n",
    "def poem_gen(model, w=None, cr=1e-1):\n",
    "    with torch.no_grad():\n",
    "        if not w in w2i or w is None:\n",
    "            idx = np.random.randint(1,n_words)\n",
    "            w = i2w[idx]\n",
    "        else:\n",
    "            idx = w2i[w]\n",
    "        ht = None\n",
    "        x0 = torch.FloatTensor([w2i['S']]).view(1,1,-1).detach()\n",
    "        x0 = idx2emb(x0)\n",
    "        y, ht = model(x0, ht)\n",
    "        x = torch.LongTensor([w2i[w]]).view(1,1,-1).detach()\n",
    "        x = idx2emb(x)\n",
    "        s = []\n",
    "        s.append(w)\n",
    "        for t in range(max_line_length):\n",
    "            y, ht = model(x, ht)\n",
    "            not_done = True\n",
    "            cnt = 0\n",
    "            while not_done and cnt <50:\n",
    "                k = min([1+np.random.binomial(3,0.5), y.size(-1)-1])\n",
    "                x = torch.topk(y, k, dim=-1)[1].detach()\n",
    "                x = x[:,:,min([np.random.geometric(0.3), k-1])].unsqueeze(2)\n",
    "#                x = torch.argmax(y,dim=-1,keepdim=True)\n",
    "                cnt += 1\n",
    "                w = batch2sent(x)\n",
    "                not_done = False\n",
    "            if w == 'E':\n",
    "                break\n",
    "            s.append(w)\n",
    "            x = idx2emb(x)\n",
    "        return u''.join(s)\n",
    "    \n",
    "    \n",
    "# 训练一个简单的 RNN 模型以生成诗歌\n",
    "\n",
    "input_size = word_emb_dim\n",
    "hidden_size = 128\n",
    "output_size = n_words\n",
    "\n",
    "model = Generator(input_size, output_size, hidden_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch0, Loss6.63741445541\n",
      "Pred:\n",
      "浪匪东东残残屿步强万微笑蹙蹙又蹙间间又头别微迷婆绝\n",
      "萧是任留临凝尽别彰起等斋念粱短琴路路昨别残短安钩粱\n",
      "Obs:\n",
      "对江北，正柳暗、愁锁千里。况一夜昙开尽，悔无计。E\n",
      "自古离情别怨，风物唯堪悲。世事同蕉鹿，谁赋式微。E\n",
      "Rnd:\n",
      "知晓壁呵偏每屿立恰却调听涛小别钩尺踏夜托灯初几损徊念性曾我怨际寥牖\n",
      "\n",
      "Epoch50, Loss5.22548151016\n",
      "Pred:\n",
      "EE。，，，。，。。。EE。，，，，。。，。。，。。E\n",
      "E。，。，，，。，，。。EE。。。。，，。。。。EE\n",
      "Obs:\n",
      "常怜香烬，终日成孤倚。忍泪付新杯，醉时看、飞云化碧。E\n",
      "百年如云梦，逆旅何匆匆。吟坐忘知闻，拈花鉴溟濛。E\n",
      "Rnd:\n",
      "岂\n",
      "\n",
      "Epoch100, Loss4.85968542099\n",
      "Pred:\n",
      "，，，，。。。。。。。EE，，，，，，。。。E\n",
      "E，，，，，，，，。。。E，，，，，，。。。。E\n",
      "Obs:\n",
      "想行迹，渐晚景、徒怀去意。留疏雨穿花底，惹香闭。E\n",
      "故旧不复忆，每酌笑愈痴。行云有秋意，知了应难知。E\n",
      "Rnd:\n",
      "章，锄鉴篷苦闲情人凝彰落，风\n",
      "\n",
      "Epoch150, Loss3.9307487011\n",
      "Pred:\n",
      "何，，，，，，，。。。。E。。，，，无无。。。。E\n",
      "何，，，，。。。。。。。EE，，，风，，。。。E\n",
      "Obs:\n",
      "问君何所适，杯酒慰客尘。鳞介天下事，走马老病身。E\n",
      "意气安可违，世路恣浮沉。且观旧物色，鹅笼共天真。E\n",
      "Rnd:\n",
      "寞。行意，独莼\n",
      "\n",
      "Epoch200, Loss3.75538110733\n",
      "Pred:\n",
      "意年，，，，，无无知。。E夜，，，，梦无。。。E\n",
      "何，，，，，无无应。意。EE，，，，，，。。。。E\n",
      "Obs:\n",
      "百年悲欢尽，朝野隔知闻。长梦有书剑，相倾无一人。E\n",
      "俯仰苍茫间，太虚应有诸。值此吟月夜，借居怀纡余。E\n",
      "Rnd:\n",
      "路排雨蕾霜阔台尽，风随飞去，风待知，离徒怀纡山新过笑，恰计俗地一陈\n",
      "\n",
      "Epoch250, Loss2.84512543678\n",
      "Pred:\n",
      "何年心事，谁无，，，笑我顽痴痴。。E肠唯有有。。。E\n",
      "百，，，，，无无应。。。E此，，，，无居怀。。。E\n",
      "Obs:\n",
      "百年心事，唤雁窗前，笑我顽痴怎寄。断肠唯有深相忆。E\n",
      "俯仰苍茫间，太虚应有诸。值此吟月夜，借居怀纡余。E\n",
      "Rnd:\n",
      "垂管。忍寂，离踟高，平、愁锁千怜谁？道管，风物闻长终，愁笑我。\n",
      "\n",
      "Epoch300, Loss2.06936240196\n",
      "Pred:\n",
      "百情眼，，暮雨难。。。E底花，，，积蒂，、、无。。。E\n",
      "百怜云际划星痕。夜清寒，谁谁陈？无意，，，无饮百年。。E\n",
      "Obs:\n",
      "浓情未已，暮雨难将息。叶底揽花痕，并蒂瘦、娇无欢意。E\n",
      "可怜云际划星痕。夜清寒，为谁陈？无意强欢，徒饮百年身。E\n",
      "Rnd:\n",
      "忍如，离往，无凭，风起，离无意年悲欢尽，春才\n",
      "\n",
      "Epoch350, Loss1.43773233891\n",
      "Pred:\n",
      "百行迹，渐晚景、徒怀去意。E疏雨穿花底，惹香闭。E\n",
      "何君何所适，杯酒慰客尘。E介天下事，走马老病身。E\n",
      "Obs:\n",
      "想行迹，渐晚景、徒怀去意。留疏雨穿花底，惹香闭。E\n",
      "问君何所适，杯酒慰客尘。鳞介天下事，走马老病身。E\n",
      "Rnd:\n",
      "忘道。夜清长客尘渺销斋云流孤柳云雁窗高楼事，萧演别排别怨。\n",
      "\n",
      "Epoch400, Loss1.63449549675\n",
      "Pred:\n",
      "百云云岩，，惊涛卷曈。。E趿隐烟，，长梦梦、。。E\n",
      "百怜云际划星痕。夜清寒，为谁陈？无意强欢，徒饮百年身。E\n",
      "Obs:\n",
      "星汉奔岩屿，惊涛卷曈虹。翕趿隐烟色，长桥海岛空。E\n",
      "可怜云际划星痕。夜清寒，为谁陈？无意强欢，徒饮百年身。E\n",
      "Rnd:\n",
      "猛笑，无穷台昙慰。\n",
      "\n",
      "Epoch450, Loss1.40152442455\n",
      "Pred:\n",
      "百羽携芳去，风风起夏池。E尽林烟，，月影入兰。。E\n",
      "百地何所悯壮士，愁回峥嵘损形骸。E山山愁空抱，，愁弦痛饮马啸哀。E\n",
      "Obs:\n",
      "灵羽携芳去，清风起夏池。夜尽林烟白，月影入兰芝。E\n",
      "天地何曾悯壮士，几回峥嵘损形骸。江山闲愁空抱冷，胡弦痛饮马啸哀。E\n",
      "Rnd:\n",
      "璧朝径意\n",
      "\n",
      "Epoch500, Loss1.040974617\n",
      "Pred:\n",
      "百年悲欢尽，朝野隔知闻。长梦有书剑，相倾无一人。E\n",
      "百汉奔岩屿，惊涛卷曈虹。翕趿隐烟色，长梦梦岛。。E\n",
      "Obs:\n",
      "百年悲欢尽，朝野隔知闻。长梦有书剑，相倾无一人。E\n",
      "星汉奔岩屿，惊涛卷曈虹。翕趿隐烟色，长桥海岛空。E\n",
      "Rnd:\n",
      "愈由气，离未渺仰茫无计。夜清卧何妨眼百尽朝颠拍宫中志每酌、碧吟尘枝\n",
      "\n",
      "Epoch550, Loss0.841648221016\n",
      "Pred:\n",
      "百古离情别怨，风物唯堪悲。世事同蕉鹿，谁赋式微。E\n",
      "百怜香烬，终日成孤倚。忍泪付新杯，醉时看、飞云化碧。E\n",
      "Obs:\n",
      "自古离情别怨，风物唯堪悲。世事同蕉鹿，谁赋式微。E\n",
      "常怜香烬，终日成孤倚。忍泪付新杯，醉时看、飞云化碧。E\n",
      "Rnd:\n",
      "排雨息尘辙风待云每，离卷由志气弛欢恨秦苑，眉眼疏。夜昙清尘辙有忆气\n",
      "\n",
      "Epoch600, Loss0.558800339699\n",
      "Pred:\n",
      "故风笑我，眉眼为谁颦。帘幕冷，素心微，咫尺殊难寄。E\n",
      "故斋即坛醮，守道安违俗。江湖得意气，狂歌岂踟躇。E\n",
      "Obs:\n",
      "海风笑我，眉眼为谁颦。帘幕冷，素心微，咫尺殊难寄。E\n",
      "心斋即坛醮，守道安违俗。江湖得意气，狂歌岂踟躇。E\n",
      "Rnd:\n",
      "行迹、锁高尽数曾华壁三成。帘即鹿，借踟才空山修息，月倚台昙峥意，衔\n",
      "\n",
      "Epoch650, Loss0.578314721584\n",
      "Pred:\n",
      "百生何所寄？天地一孤篷。郁纡且行游，迟复尘景中。E\n",
      "故拍遍，月钩锁恨秦宫苑。秦宫苑，燕歌韵歇，断云流远。E\n",
      "Obs:\n",
      "平生何所寄？天地一孤篷。郁纡且行游，迟复尘景中。E\n",
      "栏拍遍，月钩锁恨秦宫苑。秦宫苑，燕歌韵歇，断云流远。E\n",
      "Rnd:\n",
      "岂梦醒共余心不忆高。江山，风物\n",
      "\n",
      "Epoch700, Loss0.544307172298\n",
      "Pred:\n",
      "百妨笑越，何妨独越。何妨纵马摘星月。何妨醉眼、试长铗。E\n",
      "百气安可违，世路恣浮沉。且观旧物色，鹅笼共天真。E\n",
      "Obs:\n",
      "何妨笑越，何妨独越。何妨纵马摘星月。何妨醉眼、试长铗。E\n",
      "意气安可违，世路恣浮沉。且观旧物色，鹅笼共天真。E\n",
      "Rnd:\n",
      "携芳信已志月钩醒台底揽际划马老。翕眼星汉岩尘未已成书人空山疑客藏羽\n",
      "\n",
      "Epoch750, Loss0.318871825933\n",
      "Pred:\n",
      "百羽携芳去，清风起夏池。夜尽林烟白，月影入兰芝。E\n",
      "百拍遍，月钩锁恨秦宫苑。秦宫苑，燕歌韵歇，断云流远。E\n",
      "Obs:\n",
      "灵羽携芳去，清风起夏池。夜尽林烟白，月影入兰芝。E\n",
      "栏拍遍，月钩锁恨秦宫苑。秦宫苑，燕歌韵歇，断云流远。E\n",
      "Rnd:\n",
      "式微，正、徒有定声影一杯。行迹渐月意徘顽云翻，平起色长梦醒，雁窗孤\n",
      "\n",
      "Epoch800, Loss0.42815309763\n",
      "Pred:\n",
      "百排梦别，安排雨别。安排落矶千秋雪。安排山海、共清绝。E\n",
      "天心不外求，日影养虚冲。观风遣剑意，抱朴任穷通。E\n",
      "Obs:\n",
      "安排梦别，安排雨别。安排落矶千秋雪。安排山海、共清绝。E\n",
      "道心不外求，日影养虚冲。观风遣剑意，抱朴任穷通。E\n",
      "Rnd:\n",
      "欢旧恨独人\n",
      "\n",
      "Epoch850, Loss0.324555665255\n",
      "Pred:\n",
      "天山修水，昨梦渐无凭。推玉管，倾绿蚁，愁绪无消计。E\n",
      "闲年心事，唤雁窗前，笑我顽痴怎寄。断肠唯有深相忆。E\n",
      "Obs:\n",
      "远山修水，昨梦渐无凭。推玉管，倾绿蚁，愁绪无消计。E\n",
      "百年心事，唤雁窗前，笑我顽痴怎寄。断肠唯有深相忆。E\n",
      "Rnd:\n",
      "徒花积景去，萧隐色何如？云际尘埋倾谈笑未渺嵘损赋剑亦怀纡余贾生趣顽\n",
      "\n",
      "Epoch900, Loss0.300914615393\n",
      "Pred:\n",
      "百国弛山色，春华因才逐。北庭惜玉折，积风待岁除。E\n",
      "百生何所寄？天地一孤篷。郁纡且行游，迟复尘景中。E\n",
      "Obs:\n",
      "故国弛山色，春华因才逐。北庭惜玉折，积风待岁除。E\n",
      "平生何所寄？天地一孤篷。郁纡且行游，迟复尘景中。E\n",
      "Rnd:\n",
      "余心不忆月兰。行云每慰仰恨复忆三\n",
      "\n",
      "Epoch950, Loss0.376197636127\n",
      "Pred:\n",
      "百君何所适，杯酒慰客尘。鳞介天下事，走马老病身。E\n",
      "百情未已，暮雨难将息。叶底揽花痕，并蒂瘦、娇无欢意。E\n",
      "Obs:\n",
      "问君何所适，杯酒慰客尘。鳞介天下事，走马老病身。E\n",
      "浓情未已，暮雨难将息。叶底揽花痕，并蒂瘦、娇无欢意。E\n",
      "Rnd:\n",
      "堂去百寻客尘台。夜高，眉由\n",
      "\n",
      "Epoch1000, Loss0.322808295488\n",
      "Pred:\n",
      "百妨笑越，何妨独越。何妨纵马摘星月。何妨醉眼、试长铗。E\n",
      "百景或可悲，愁予感韫椟。明日放归去，长梦酬三馀。E\n",
      "Obs:\n",
      "何妨笑越，何妨独越。何妨纵马摘星月。何妨醉眼、试长铗。E\n",
      "浮景或可悲，愁予感韫椟。明日放归去，长梦酬三馀。E\n",
      "Rnd:\n",
      "平笑越何所，离踟寄旧不外徘间，暮生趣虹慰客尘台一笑愈香尽朝颠云远山\n",
      "\n",
      "Epoch1050, Loss0.230038002133\n",
      "Pred:\n",
      "百风笑我，眉眼为谁颦。帘幕冷，素心微，咫尺殊难寄。E\n",
      "百雨息尘心，锄归庭扫迟。空山疑过客，折竹两三枝。E\n",
      "Obs:\n",
      "海风笑我，眉眼为谁颦。帘幕冷，素心微，咫尺殊难寄。E\n",
      "久雨息尘心，锄归庭扫迟。空山疑过客，折竹两三枝。E\n",
      "Rnd:\n",
      "妨醉看、徒恨，了每赋谁雁去。行迹去？平、徒饮百年悲。夜昙花生幽寥碧\n",
      "\n",
      "Epoch1100, Loss0.216737046838\n",
      "Pred:\n",
      "百年悲欢尽，朝野隔知闻。长梦有书剑，相倾无一人。E\n",
      "百阳台昙花三五成蕾，似欲娇绽，余心甚慰，常与之对坐倾谈。E\n",
      "Obs:\n",
      "百年悲欢尽，朝野隔知闻。长梦有书剑，相倾无一人。E\n",
      "见阳台昙花三五成蕾，似欲娇绽，余心甚慰，常与之对坐倾谈。E\n",
      "Rnd:\n",
      "樽古一清剑寄？风起池夜昙花老病芳恨年心事知海，常之坐谈气添，平贾生\n",
      "\n",
      "Epoch1150, Loss0.227877914906\n",
      "Pred:\n",
      "百怜云流随身老，是非帝业终尘埋。忍将寂寞立长夜，梦里狂沙挟月来。E\n",
      "百风笑我，眉眼为谁颦。帘幕冷，素心微，咫尺殊难寄。E\n",
      "Obs:\n",
      "可怜风流随身老，是非帝业终尘埋。忍将寂寞立长夜，梦里狂沙挟月来。E\n",
      "海风笑我，眉眼为谁颦。帘幕冷，素心微，咫尺殊难寄。E\n",
      "Rnd:\n",
      "曾或悲世色清玉折谢几人。帘即奔悯士违。\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-f567843ef43d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0my_obs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_obs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_obs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mLoss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/torch/tensor.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m \u001b[0;34m`\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/torch/autograd/__init__.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lr = 1e-3\n",
    "n_epochs = 20000\n",
    "last_epoch = -1\n",
    "disp_interval = 50\n",
    "batch_size = 2\n",
    "\n",
    "loss_func = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n",
    "\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "def lr_lambda(epoch):\n",
    "    return 0.99**(epoch/50.0)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch=last_epoch)\n",
    "\n",
    "#model.load_state_dict(torch.load('saves/model-ernn.pt'))\n",
    "\n",
    "Loss = []\n",
    "for epoch in range(n_epochs):\n",
    "    model.zero_grad()\n",
    "    x_obs, y_obs = get_batch(batch_size=batch_size)\n",
    "    x_obs = idx2emb(x_obs)\n",
    "    y_pred, ht = model(x_obs)\n",
    "    y1 = torch.argmax(y_pred.detach(),-1,keepdim=True).detach()#[:,:1,:]\n",
    "    y2 = y_obs.detach()#[:,:1,:]\n",
    "    y_pred = y_pred.view(-1,output_size)\n",
    "    y_obs = y_obs.contiguous().view(-1)\n",
    "    loss = loss_func(y_pred,y_obs)\n",
    "    loss.backward()\n",
    "    Loss.append(loss.tolist())\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    if epoch % disp_interval == 0:\n",
    "        print(u'Epoch{}, Loss{}\\nPred:\\n{}\\nObs:\\n{}\\nRnd:\\n{}\\n'.format(epoch,loss.tolist(), batch2sent(y1), batch2sent(y2),poem_gen(model)))\n",
    "        torch.save(model.state_dict(),'saves/model-ernn.pt')\n",
    "window_size = 50\n",
    "avg_losses = np.array(Loss)[:len(Loss)//50 *50].reshape([-1,window_size]).mean(1)\n",
    "pl.plot(np.arange(0,len(Loss)//50 *50,window_size), avg_losses,'r-')\n",
    "pl.xlabel('Time')\n",
    "pl.ylabel('Loss')\n",
    "pl.yscale('log')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1807,  0.6392,  0.1268,  1.1103, -1.6153, -0.1511, -0.0435,\n",
       "        -0.0471, -0.8821, -0.4894, -0.1626,  0.1210,  1.6830, -0.1591,\n",
       "        -0.0498, -0.0988,  0.3077,  1.0061,  0.2484, -0.7002])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embeddings.weight[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "252450"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "33*7650"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 3])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([torch.zeros(4,3).unsqueeze(0),torch.zeros(4,3).unsqueeze(0)]).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "16661700/7650/33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

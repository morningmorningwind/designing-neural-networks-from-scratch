{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入模块\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "from torch import nn\n",
    "from nltk.translate import bleu_score\n",
    "\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个简单RNN层\n",
    "class ERNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(ERNN,self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.ih_linear = nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.hh_linear = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        \n",
    "    def init_h(self, x):\n",
    "        self.ht = torch.randn_like(x[0])\n",
    "    \n",
    "    def forward(self, x, h=None):\n",
    "        if h is None:\n",
    "            self.init_h(x)\n",
    "        seq_length, batch_size, input_size = x.size()\n",
    "        y = []\n",
    "        for t in range(seq_length):\n",
    "            self.ht = torch.tanh(self.ih_linear(x[t]) + self.hh_linear(self.ht))\n",
    "            y.append(self.ht.unsqueeze(0))\n",
    "        y = torch.cat(y)\n",
    "        return y, self.ht\n",
    "\n",
    "# 定义一个LSTM层\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(LSTM,self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.ii_linear = nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.hi_linear = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.if_linear = nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.hf_linear = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.ig_linear = nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.hg_linear = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.io_linear = nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.ho_linear = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "    def init_h(self, x):\n",
    "        self.ht = torch.randn_like(x[0])\n",
    "    def init_c(self, x):\n",
    "        self.ct = torch.randn_like(x[0])\n",
    "    \n",
    "    def forward(self, x, h=None, c=None):\n",
    "        if h is None:\n",
    "            self.init_h(x)\n",
    "        if c is None:\n",
    "            self.init_c(x)\n",
    "        seq_length, batch_size, input_size = x.size()\n",
    "        y = []\n",
    "        for t in range(seq_length):\n",
    "            it = torch.sigmoid(self.ii_linear(x[t]) + self.hi_linear(self.ht))\n",
    "            ft = torch.sigmoid(self.if_linear(x[t]) + self.hf_linear(self.ht))\n",
    "            gt = torch.tanh(self.ig_linear(x[t]) + self.hg_linear(self.ht))\n",
    "            ot = torch.sigmoid(self.io_linear(x[t]) + self.ho_linear(self.ht))\n",
    "            self.ct = ft * self.ct + it * gt\n",
    "            self.ht = ot * torch.tanh(self.ct)\n",
    "            y.append(self.ht.unsqueeze(0))\n",
    "        y = torch.cat(y)\n",
    "        return y, self.ht\n",
    "\n",
    "    \n",
    "# 定义一个GRU层\n",
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(GRU,self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.in_linear = nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.hn_linear = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.ir_linear = nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.hr_linear = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.iz_linear = nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.hz_linear = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "    def init_h(self, x):\n",
    "        self.ht = torch.randn_like(x[0])\n",
    "    \n",
    "    def forward(self, x, h=None):\n",
    "        if h is None:\n",
    "            self.init_h(x)\n",
    "        seq_length, batch_size, input_size = x.size()\n",
    "        y = []\n",
    "        for t in range(seq_length):\n",
    "            rt = torch.sigmoid(self.ir_linear(x[t]) + self.hr_linear(self.ht))\n",
    "            zt = torch.sigmoid(self.iz_linear(x[t]) + self.hz_linear(self.ht))\n",
    "            nt = torch.tanh(self.in_linear(x[t]) + rt * self.hn_linear(self.ht))\n",
    "            self.ht = (1 - zt) * nt + zt * self.ht\n",
    "            y.append(self.ht.unsqueeze(0))\n",
    "        y = torch.cat(y)\n",
    "        return y, self.ht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data summary:\n",
      "\n",
      " number of poems: 42\n",
      " number of words: 773\n",
      "\n",
      "Poem examples:\n",
      "\n",
      "S平生何所寄？天地一孤篷。郁纡且行游，迟复尘景中。E\n",
      "S星汉奔岩屿，惊涛卷曈虹。翕趿隐烟色，长桥海岛空。E\n",
      "S百年如云梦，逆旅何匆匆。吟坐忘知闻，拈花鉴溟濛。E\n",
      "S道心不外求，日影养虚冲。观风遣剑意，抱朴任穷通。E\n",
      "S千古一杯清，卧剑亦何如？云雁有芳信，谈笑未成书。E\n",
      "S故国弛山色，春华因才逐。北庭惜玉折，积风待岁除。E\n",
      "S俯仰苍茫间，太虚应有诸。值此吟月夜，借居怀纡余。E\n",
      "S心斋即坛醮，守道安违俗。江湖得意气，狂歌岂踟躇。E\n",
      "S所忧非尘辙，萧萧演六虚。冷眼任霜雪，平生性慵疏。E\n",
      "S浮景或可悲，愁予感韫椟。明日放归去，长梦酬三馀。E\n"
     ]
    }
   ],
   "source": [
    "# 训练一个基于ERNN神经网络来作诗\n",
    "\n",
    "## 读入用GloVe处理得到的文字 embeddings，以及句子数据。\n",
    "import codecs\n",
    "\n",
    "with codecs.open('data/word_embeddings_manyun_128.txt', mode='r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "n_words = len(lines) + 1\n",
    "word_emb_dim = input_size = 128\n",
    "word_embeddings = torch.nn.Embedding(n_words, word_emb_dim)\n",
    "i2w = {0:''}\n",
    "w2i = {'':0}\n",
    "for i in range(0, n_words - 1):\n",
    "    line = lines[i].split(' ')\n",
    "    i2w[i + 1] = line[0]\n",
    "    w2i[line[0]] = i + 1\n",
    "#    word_embeddings.weight[i] = torch.from_numpy(np.array(line[1:],dtype=np.float32))\n",
    "\n",
    "word_embeddings.weight.require_grad = False\n",
    "\n",
    "poems = []\n",
    "max_line_length = 32\n",
    "with codecs.open('data/manyun.txt', mode='r', encoding='utf-8') as f:\n",
    "    for poem in f:\n",
    "        poem = poem.replace(' ','')\n",
    "        if ':' in poem: poem = poem.split(':')[-1]\n",
    "        poem = poem.replace('\\n','')\n",
    "        poem = poem.replace('\\r','')\n",
    "        if len(poem) < 24 or len(poem) > max_line_length or '(' in poem or u'（' in poem or u'）' in poem or ')' in poem:\n",
    "            continue\n",
    "        poem = 'S' + poem + 'E'\n",
    "        poems.append(map(w2i.get, poem))\n",
    "\n",
    "n_poems = len(poems)\n",
    "\n",
    "print( 'Data summary:\\n\\n number of poems: {}\\n number of words: {}\\n'.format(n_poems, n_words))\n",
    "print('Poem examples:\\n\\n'+'\\n'.join([''.join(map(i2w.get, x)) for x in poems[:10]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S已由志气添白发，未因多战负情怀。力微敢弃终军骨，栖迟更难忘劳徕。\n",
      "已由志气添白发，未因多战负情怀。力微敢弃终军骨，栖迟更难忘劳徕。E\n"
     ]
    }
   ],
   "source": [
    "# 定义一个函数，随机返回一个 mini batch，用于训练，由于每一首诗歌的长度不同，我们此处规定每个batch只有一首诗。这样，就可以生成长度可变的诗歌。\n",
    "def get_batch(batch_size=1):\n",
    "    batch_raw = [poems[i][:] for i in np.random.randint(0, n_poems, batch_size)]\n",
    "    max_length = max(map(len, batch_raw))\n",
    "    for i in range(len(batch_raw)):\n",
    "        for j in range(len(batch_raw[i]),max_length):\n",
    "            batch_raw[i].append(w2i[''])\n",
    "    batch_raw = torch.LongTensor(batch_raw).detach().unsqueeze(2).transpose(0,1)\n",
    "    x = batch_raw[:-1].type(torch.float32)\n",
    "    y = batch_raw[1:]\n",
    "    return x, y\n",
    "\n",
    "def idx2emb(x):\n",
    "    return word_embeddings(x.type(torch.long)).squeeze(2).detach()\n",
    "    \n",
    "\n",
    "# 定义一个函数，输入一个 batch 返回句子\n",
    "def batch2sent(batch, aslist=False):\n",
    "    S = []\n",
    "    batch = batch.type(torch.int32).detach()\n",
    "    seq_length, batch_size, emb_size = batch.size()\n",
    "    for i in range(batch_size):\n",
    "        S.append(''.join(map(i2w.get, batch[:,i,:].view(-1).tolist())))\n",
    "    if not aslist: \n",
    "        S = u'\\n'.join(S)\n",
    "    return S\n",
    "\n",
    "x, y = get_batch(1)\n",
    "print(batch2sent(x))\n",
    "print(batch2sent(y))\n",
    "\n",
    "def batch_bleu(batch_data, batch_pred):\n",
    "    batch_data = map(lambda x:[x], map(list, batch_data))\n",
    "    batch_pred = map(list, batch_pred)\n",
    "    return bleu_score.corpus_bleu(batch_data, batch_pred)\n",
    "    \n",
    "\n",
    "# 定义一个生成器\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = LSTM(self.input_size, self.hidden_size)\n",
    "        self.output = nn.Linear(self.hidden_size, self.output_size)\n",
    "        self.logsoftmax = torch.nn.LogSoftmax(dim=-1)\n",
    "    def forward(self, x, h0=None):\n",
    "        seq_length, batch_size, input_size = x.size()\n",
    "        y, ht = self.rnn(x, h0)\n",
    "        y = y.view(-1, self.hidden_size)\n",
    "        y = self.output(y)\n",
    "        y = y.view(seq_length, batch_size, output_size)\n",
    "        y = self.logsoftmax(y)\n",
    "        return y, ht\n",
    "\n",
    "def poem_gen(model, w=None, cr=1e-1):\n",
    "    with torch.no_grad():\n",
    "        if not w in w2i or w is None:\n",
    "            idx = np.random.randint(1,n_words)\n",
    "            w = i2w[idx]\n",
    "        else:\n",
    "            idx = w2i[w]\n",
    "        ht = None\n",
    "        x0 = torch.FloatTensor([w2i['S']]).view(1,1,-1).detach()\n",
    "        x0 = idx2emb(x0)\n",
    "        y, ht = model(x0, ht)\n",
    "        x = torch.LongTensor([w2i[w]]).view(1,1,-1).detach()\n",
    "        x = idx2emb(x)\n",
    "        s = []\n",
    "        s.append(w)\n",
    "        for t in range(max_line_length):\n",
    "            y, ht = model(x, ht)\n",
    "            not_done = True\n",
    "            cnt = 0\n",
    "            while not_done and cnt <50:\n",
    "                k = min([1+np.random.binomial(3,0.5), y.size(-1)-1])\n",
    "                x = torch.topk(y, k, dim=-1)[1].detach()\n",
    "                x = x[:,:,min([np.random.geometric(0.3), k-1])].unsqueeze(2)\n",
    "#                x = torch.argmax(y,dim=-1,keepdim=True)\n",
    "                cnt += 1\n",
    "                w = batch2sent(x)\n",
    "                not_done = False\n",
    "            if w == 'E':\n",
    "                break\n",
    "            s.append(w)\n",
    "            x = idx2emb(x)\n",
    "        return u''.join(s)\n",
    "    \n",
    "    \n",
    "# 训练一个简单的 RNN 模型以生成诗歌\n",
    "\n",
    "input_size = word_emb_dim\n",
    "hidden_size = 128\n",
    "output_size = n_words\n",
    "\n",
    "model = Generator(input_size, output_size, hidden_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch0, Loss0.165372759104, BLEU0.96, \n",
      "Pred:\n",
      "百江北，正柳暗、愁锁千里。况一夜昙开尽，悔无计。E\n",
      "百古离情别怨，风物唯堪悲。世事同蕉鹿，谁赋式微。E\n",
      "Obs:\n",
      "对江北，正柳暗、愁锁千里。况一夜昙开尽，悔无计。E\n",
      "自古离情别怨，风物唯堪悲。世事同蕉鹿，谁赋式微。E\n",
      "Rnd:\n",
      "知闻，谁陈雁临崖一人昙花无端，折谁堪地何所？雁窗色，却其寥石痴行云\n",
      "\n",
      "Epoch50, Loss0.15601336956, BLEU0.98, \n",
      "Pred:\n",
      "百怜香烬，终日成孤倚。忍泪付新杯，醉时看、飞云化碧。E\n",
      "百年如云梦，逆旅何匆匆。吟坐忘知闻，拈花鉴溟濛。E\n",
      "Obs:\n",
      "常怜香烬，终日成孤倚。忍泪付新杯，醉时看、飞云化碧。E\n",
      "百年如云梦，逆旅何匆匆。吟坐忘知闻，拈花鉴溟濛。E\n",
      "Rnd:\n",
      "岂踟躇，正眼、旧物。值山闲步气笑，急余心事雁窗情伤凭痴绝，风物梦\n",
      "\n",
      "Epoch100, Loss0.163215786219, BLEU0.96, \n",
      "Pred:\n",
      "百江北，正柳暗、愁锁千里。况一夜昙开尽，悔无计。E\n",
      "百欢有时尽，天地无穷极。尽数养五藏，造化由心生。E\n",
      "Obs:\n",
      "对江北，正柳暗、愁锁千里。况一夜昙开尽，悔无计。E\n",
      "悲欢有时尽，天地无穷极。尽数养五藏，造化由心生。E\n",
      "Rnd:\n",
      "劳徕羽难忘机晚微敢终日放弛我顽，急行有时敢雨难，几瘦？无凭欢云际花\n",
      "\n",
      "Epoch150, Loss0.110176548362, BLEU0.98, \n",
      "Pred:\n",
      "故风笑我，眉眼为谁颦。帘幕冷，素心微，咫尺殊难寄。E\n",
      "可怜风流随身老，是非帝业终尘埋。忍将寂寞立长夜，梦里狂沙挟月来。E\n",
      "Obs:\n",
      "海风笑我，眉眼为谁颦。帘幕冷，素心微，咫尺殊难寄。E\n",
      "可怜风流随身老，是非帝业终尘埋。忍将寂寞立长夜，梦里狂沙挟月来。E\n",
      "Rnd:\n",
      "横心即，玉\n",
      "\n",
      "Epoch200, Loss0.133856222034, BLEU0.96, \n",
      "Pred:\n",
      "远欢有时尽，天地无穷极。尽数养五藏，造化由心生。E\n",
      "远野颠踣迷故路，过眼星云翻盛衰。赤壁有定空余恨，衔石痴绝误贾才。E\n",
      "Obs:\n",
      "悲欢有时尽，天地无穷极。尽数养五藏，造化由心生。E\n",
      "朝野颠踣迷故路，过眼星云翻盛衰。赤壁有定空余恨，衔石痴绝误贾才。E\n",
      "Rnd:\n",
      "惭养虚应难将意，妨趣余排雨难忘机任病江湖得意，栖劳徕独越。长梦别，\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-0b9f1dd7f966>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0my_obs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_obs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_obs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mLoss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/torch/tensor.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m \u001b[0;34m`\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/torch/autograd/__init__.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lr = 1e-3\n",
    "n_epochs = 2000\n",
    "last_epoch = -1\n",
    "disp_interval = 50\n",
    "batch_size = 2\n",
    "\n",
    "loss_func = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n",
    "\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "def lr_lambda(epoch):\n",
    "    return 0.99**(epoch/50.0)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch=last_epoch)\n",
    "\n",
    "#model.load_state_dict(torch.load('saves/model-ernn.pt'))\n",
    "\n",
    "Loss = []\n",
    "BLEU = []\n",
    "for epoch in range(n_epochs):\n",
    "    model.zero_grad()\n",
    "    x_obs, y_obs = get_batch(batch_size=batch_size)\n",
    "    x_obs = idx2emb(x_obs)\n",
    "    y_pred, ht = model(x_obs)\n",
    "    y1 = torch.argmax(y_pred.detach(),-1,keepdim=True).detach()#[:,:1,:]\n",
    "    y2 = y_obs.detach()#[:,:1,:]\n",
    "    y_pred = y_pred.view(-1,output_size)\n",
    "    y_obs = y_obs.contiguous().view(-1)\n",
    "    loss = loss_func(y_pred,y_obs)\n",
    "    loss.backward()\n",
    "    Loss.append(loss.tolist())\n",
    "\n",
    "    batch_data, batch_pred = batch2sent(y1), batch2sent(y2)\n",
    "    bleu = batch_bleu(batch_data.split('\\n'), batch_pred.split('\\n'))\n",
    "    BLEU.append(bleu)\n",
    "    \n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    if epoch % disp_interval == 0:\n",
    "        print(u'Epoch{}, Loss{}, BLEU{}, \\nPred:\\n{}\\nObs:\\n{}\\nRnd:\\n{}\\n'.format(epoch,loss.tolist(),round(BLEU[-1],2), batch_data, batch_pred,poem_gen(model)))\n",
    "        torch.save(model.state_dict(),'saves/model-ernn.pt')\n",
    "fig = pl.figure(1)\n",
    "window_size = 50\n",
    "avg_losses = np.array(Loss)[:len(Loss)//50 *50].reshape([-1,window_size]).mean(1)\n",
    "pl.plot(np.arange(0,len(Loss)//50 *50,window_size), avg_losses,'r-')\n",
    "pl.xlabel('Time')\n",
    "pl.ylabel('Loss')\n",
    "pl.yscale('log')\n",
    "\n",
    "fig = pl.figure(2)\n",
    "window_size = 50\n",
    "avg_losses = np.array(BLEU)[:len(BLEU)//50 *50].reshape([-1,window_size]).mean(1)\n",
    "pl.plot(np.arange(0,len(BLEU)//50 *50,window_size), avg_losses,'r-')\n",
    "pl.xlabel('Time')\n",
    "pl.ylabel('BLEU')\n",
    "pl.yscale('log')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1807,  0.6392,  0.1268,  1.1103, -1.6153, -0.1511, -0.0435,\n",
       "        -0.0471, -0.8821, -0.4894, -0.1626,  0.1210,  1.6830, -0.1591,\n",
       "        -0.0498, -0.0988,  0.3077,  1.0061,  0.2484, -0.7002])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embeddings.weight[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[u'\\u767e',\n",
       "   u'\\u9645',\n",
       "   u'\\u4f55',\n",
       "   u'\\u66fe',\n",
       "   u'\\u60af',\n",
       "   u'\\u58ee',\n",
       "   u'\\u58eb',\n",
       "   u'\\uff0c',\n",
       "   u'\\u51e0',\n",
       "   u'\\u56de',\n",
       "   u'\\u5ce5',\n",
       "   u'\\u5d58',\n",
       "   u'\\u635f',\n",
       "   u'\\u5f62',\n",
       "   u'\\u9ab8',\n",
       "   u'\\u3002',\n",
       "   u'\\u6c5f',\n",
       "   u'\\u5c71',\n",
       "   u'\\u95f2',\n",
       "   u'\\u6101',\n",
       "   u'\\u7a7a',\n",
       "   u'\\u62b1',\n",
       "   u'\\u51b7',\n",
       "   u'\\uff0c',\n",
       "   u'\\u80e1',\n",
       "   u'\\u5f26',\n",
       "   u'\\u75db',\n",
       "   u'\\u996e',\n",
       "   u'\\u9a6c',\n",
       "   u'\\u5578',\n",
       "   u'\\u54c0',\n",
       "   u'\\u3002',\n",
       "   u'E']],\n",
       " [[u'\\u767e',\n",
       "   u'\\u6c5f',\n",
       "   u'\\u5317',\n",
       "   u'\\uff0c',\n",
       "   u'\\u6b63',\n",
       "   u'\\u67f3',\n",
       "   u'\\u6697',\n",
       "   u'\\u3001',\n",
       "   u'\\u6101',\n",
       "   u'\\u9501',\n",
       "   u'\\u5343',\n",
       "   u'\\u91cc',\n",
       "   u'\\u3002',\n",
       "   u'\\u51b5',\n",
       "   u'\\u4e00',\n",
       "   u'\\u591c',\n",
       "   u'\\u6619',\n",
       "   u'\\u5f00',\n",
       "   u'\\u5c3d',\n",
       "   u'\\uff0c',\n",
       "   u'\\u6094',\n",
       "   u'\\u65e0',\n",
       "   u'\\u8ba1',\n",
       "   u'\\u3002',\n",
       "   u'E']]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map(lambda x:[x], map(list, batch_data.split('\\n')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_bleu(batch_data.split('\\n'), batch_data.split('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "16661700/7650/33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b = 1,2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

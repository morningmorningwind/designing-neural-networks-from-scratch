{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入模块\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload\n",
    "import unicodecsv as csv\n",
    "import codecs\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import pylab as pl\n",
    "torch.set_default_tensor_type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(201, 717)\n"
     ]
    }
   ],
   "source": [
    "# 导入数据\n",
    "'''\n",
    "max_sentence_length = 10\n",
    "\n",
    "data_size = 200\n",
    "\n",
    "# indexing the words\n",
    "source_sentences = []\n",
    "w2i_s = {'':0}\n",
    "i2w_s = {0:''}\n",
    "\n",
    "target_sentences = []                \n",
    "w2i_t = {'':0}\n",
    "i2w_t = {0:''}\n",
    "\n",
    "n1 = 1\n",
    "n2 = 1\n",
    "with codecs.open('data/chat.txt','r') as f:\n",
    "    data = csv.reader(f,delimiter='\\t')\n",
    "    for row in data:\n",
    "        s1 = row[0].split(' ')[:max_sentence_length]\n",
    "        s2 = row[1].split(' ')[:max_sentence_length]\n",
    "        source_sentences.append(s1)\n",
    "        target_sentences.append(s2)\n",
    "\n",
    "        for w in s1:\n",
    "            if not w in w2i_s: \n",
    "                w2i_s[w] = n1\n",
    "                i2w_s[n1] = w\n",
    "                n1 += 1\n",
    "        for w in s2:\n",
    "            if not w in w2i_t: \n",
    "                w2i_t[w] = n2\n",
    "                i2w_t[n2] = w\n",
    "                n2 += 1\n",
    "        if len(target_sentences)>data_size:\n",
    "            break\n",
    "\n",
    "n_words_source = len(w2i_s)\n",
    "n_words_target = len(w2i_t)\n",
    "n_words = n_words_source + n_words_target\n",
    "print(len(target_sentences), n_words)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9900, 1201)\n"
     ]
    }
   ],
   "source": [
    "# 导入数据\n",
    "max_sentence_length = 30\n",
    "\n",
    "#data_size = 200\n",
    "\n",
    "# indexing the words\n",
    "source_sentences = []\n",
    "w2i_s = {'':0}\n",
    "i2w_s = {0:''}\n",
    "\n",
    "target_sentences = []                \n",
    "w2i_t = {'':0}\n",
    "i2w_t = {0:''}\n",
    "\n",
    "n1 = 1\n",
    "with codecs.open('data/training-es.txt','r') as f:\n",
    "    data = csv.reader(f,delimiter=' ')\n",
    "    for row in data:\n",
    "        s1 = row[:max_sentence_length]\n",
    "        source_sentences.append(s1)\n",
    "        for w in s1:\n",
    "            if not w in w2i_s: \n",
    "                w2i_s[w] = n1\n",
    "                i2w_s[n1] = w\n",
    "                n1 += 1\n",
    "\n",
    "n2 = 1\n",
    "with codecs.open('data/training-en.txt','r') as f:\n",
    "    data = csv.reader(f,delimiter=' ')\n",
    "    for row in data:\n",
    "        s2 = row[:max_sentence_length]\n",
    "        target_sentences.append(s2)\n",
    "        for w in s2:\n",
    "            if not w in w2i_t: \n",
    "                w2i_t[w] = n2\n",
    "                i2w_t[n2] = w\n",
    "                n2 += 1\n",
    "            \n",
    "data_size = len(target_sentences)\n",
    "n_words_source = len(w2i_s)\n",
    "n_words_target = len(w2i_t)\n",
    "n_words = n_words_source + n_words_target\n",
    "print(data_size, n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding using skip gram, i.e., the nearest\n",
    "word_embedding_dim = 15\n",
    "sentence_embedding_dim = word_embedding_dim * max_sentence_length\n",
    "word_embeddings_source = nn.Embedding(n_words_source, word_embedding_dim, max_norm=1).data.zero_()\n",
    "word_embeddings_target = nn.Embedding(n_words_target, word_embedding_dim, max_norm=1).data.zero_()\n",
    "\n",
    "#A_source = np.diag(np.ones(nw_source))\n",
    "#A_target = np.diag(np.ones(nw_target))\n",
    "for row in source_sentences:\n",
    "    for i in range(0,max_sentence_length):\n",
    "#        A_source[W2I_source[row[i]],W2I_source[row[i]]] += 1.0\n",
    "#        A_source[W2I_source[row[i+1]],W2I_source[row[i]]] += 1.0\n",
    "        if i < len(row)+2:\n",
    "            if i == len(row) -1:\n",
    "                word_embeddings_source[w2i_s[row[i]],w2i_s['']] += 1.0\n",
    "            elif i > len(row) -1:\n",
    "                word_embeddings_source[w2i_s[''],w2i_s['']] += 1.0\n",
    "            else:\n",
    "                word_embeddings_source[w2i_s[row[i]],w2i_s[row[i+1]]] += 1.0\n",
    "\n",
    "decay_rate = 1.0\n",
    "\n",
    "for i in range(A_source.shape[0]):\n",
    "    A_source[i,:] = A_source[i,:]/np.linalg.norm(A_source[i,:])\n",
    "    \n",
    "def series(A,n=2,beta=0.5):\n",
    "    A += beta * A.dot(A)\n",
    "    return A\n",
    "\n",
    "#A_source = series(A_source)\n",
    "A_source = expm(decay_rate*A_source)\n",
    "np.fill_diagonal(A_source,0)\n",
    "#A_source=np.triu(A_source,1)\n",
    "#A_source = sigmoid(A_source)\n",
    "#A_source = np.linalg.pinv(np.diag(np.ones(nw_source))-decay_rate*A_source)\n",
    "\n",
    "for row in Target:\n",
    "    for i in range(0,maxL_target):\n",
    "#        A_target[W2I_target[row[i]],W2I_target[row[i]]] += 1.0\n",
    "#        A_target[W2I_target[row[i+1]],W2I_target[row[i]]] += 1.0\n",
    "        if i < len(row)+1:\n",
    "            if i == len(row) -1:\n",
    "                A_target[W2I_target[row[i]],W2I_target['']] += 1.0\n",
    "            elif i > len(row) -1:\n",
    "                A_target[W2I_target[''],W2I_target['']] += 1.0\n",
    "            else:\n",
    "                A_target[W2I_target[row[i]],W2I_target[row[i+1]]] += 1.0\n",
    "                \n",
    "for i in range(A_target.shape[0]):\n",
    "    A_target[i,:] = A_target[i,:]/np.linalg.norm(A_target[i,:])\n",
    "    \n",
    "#A_target = series(A_target)\n",
    "  \n",
    "A_target = expm(decay_rate*A_target)\n",
    "np.fill_diagonal(A_target,0)\n",
    "#A_target=np.triu(A_target,1)\n",
    "#A_target = sigmoid(A_target) \n",
    "#A_target = np.linalg.pinv(np.diag(np.ones(nw_target))-decay_rate*A_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 词向量表示\n",
    "word_embedding_dim = 15\n",
    "sentence_embedding_dim = word_embedding_dim * max_sentence_length\n",
    "word_embeddings_source = nn.Embedding(n_words_source, word_embedding_dim, max_norm=1)\n",
    "word_embeddings_target = nn.Embedding(n_words_target, word_embedding_dim, max_norm=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个深层神经网络\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size, n_hidden=1, activation=None, out_activation=None, enable_dropout=False):\n",
    "        super(DNN,self).__init__()\n",
    "        self.input = nn.Linear(input_size, hidden_size)\n",
    "        hiddens = []\n",
    "        self.n_hidden = n_hidden\n",
    "        self.activation = activation\n",
    "        self.enable_dropout = enable_dropout\n",
    "        self.dropout = nn.Dropout(p=0.05)\n",
    "        for n in range(self.n_hidden):\n",
    "            hiddens.append(nn.Linear(hidden_size, hidden_size))\n",
    "        self.hiddens = nn.Sequential(*hiddens)\n",
    "        self.output = nn.Linear(hidden_size, output_size)\n",
    "        self.out_activation = out_activation\n",
    "    def forward(self, x):\n",
    "        x = self.input(x)\n",
    "        if self.activation is not None:\n",
    "            x = self.activation(x)\n",
    "        for i in range(self.n_hidden):\n",
    "            x = self.hiddens[i](x)\n",
    "            if self.enable_dropout:\n",
    "                self.dropout(x)\n",
    "            if self.activation is not None:\n",
    "                x = self.activation(x)\n",
    "        x = self.output(x)\n",
    "        if self.out_activation is not None:\n",
    "            x = self.out_activation(x).clamp(1e-8,1-1e-7)\n",
    "        return x\n",
    "\n",
    "# LSTM\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, seq_size, batch_size, input_size, hidden_size, n_hidden=1, in_layers=None, out_layers=None):\n",
    "        super(LSTM,self).__init__()\n",
    "        self.in_layers = in_layers\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_hidden = n_hidden\n",
    "        self.seq_size = seq_size\n",
    "        self.batch_size = batch_size\n",
    "        self.lstm = nn.LSTM(input_size, self.hidden_size, num_layers=n_hidden)\n",
    "        self.out_layers = out_layers\n",
    "#        self.init_hidden()\n",
    "    \n",
    "#    def init_hidden(self):\n",
    "#        self.hx =(torch.randn(self.n_hidden, self.batch_size, self.hidden_size), torch.randn(self.n_hidden, self.batch_size, self.hidden_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.batch_size = x.size(0)\n",
    "#        self.hx=self.init_hidden()\n",
    "        if self.in_layers is not None:\n",
    "            x = self.in_layers(x)\n",
    "            x = x.contiguous().view(self.batch_size, self.seq_size, self.input_size)\n",
    "        x = torch.transpose(x,0,1)\n",
    "        x, hx = self.lstm(x)\n",
    "        x = torch.transpose(x,0,1)\n",
    "        if self.out_layers is not None:\n",
    "            x = x.contiguous().view(self.batch_size,-1)\n",
    "            x = self.out_layers(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "# 定义一个函数，将输入句子文字转化为矢量编码\n",
    "def get_sentence_embedding(S,mode='source'):\n",
    "    if mode == 'source':\n",
    "        idx = [w2i_s[w] for w in S]\n",
    "        for i in range(len(S), max_sentence_length):\n",
    "            idx.append(0)\n",
    "        idx = torch.LongTensor([idx])\n",
    "        return word_embeddings_source(idx)#.view(1,-1)\n",
    "    elif mode == 'target':\n",
    "        idx = [w2i_t[w] for w in S]\n",
    "        for i in range(len(S), max_sentence_length):\n",
    "            idx.append(0)\n",
    "        idx = torch.LongTensor([idx])\n",
    "        return word_embeddings_target(idx)#.view(1,-1)\n",
    "    else:\n",
    "        print('mode error.')\n",
    "        return None\n",
    "\n",
    "    \n",
    "def target_sentence_embedding2words(embs):\n",
    "    embs = embs.view(max_sentence_length,-1)\n",
    "    words = []\n",
    "    for i in range(max_sentence_length):\n",
    "        loss = torch.mean((word_embeddings_target.weight.detach()-embs[i].detach())**2,1)\n",
    "        words.append(i2w_t[torch.argmin(loss).tolist()])\n",
    "    return u' '.join(words)\n",
    "\n",
    "def source_sentence_embedding2words(embs):\n",
    "    embs = embs.view(max_sentence_length,-1)\n",
    "    words = []\n",
    "    for i in range(max_sentence_length):\n",
    "        loss = torch.mean((word_embeddings_source.weight.detach()-embs[i].detach())**2,1)\n",
    "        words.append(i2w_s[torch.argmin(loss).tolist()])\n",
    "    return u' '.join(words)\n",
    "\n",
    "def get_loser(loss_g=None,loss_d=None):\n",
    "    if loss_g > loss_d:\n",
    "        return 'G'\n",
    "    elif loss_g < loss_d:\n",
    "        return 'D'\n",
    "    else:\n",
    "        if np.random.random()<0.5:\n",
    "            return 'G'\n",
    "        else:\n",
    "            return 'D'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型参数设置与模型\n",
    "\n",
    "\n",
    "dnn_input_size = max_sentence_length* word_embedding_dim\n",
    "dnn_output_size = 1\n",
    "dnn_hidden_size = 64\n",
    "dnn_activation = torch.relu\n",
    "\n",
    "batch_size = 5\n",
    "\n",
    "\n",
    "gnn_input_size = 20\n",
    "gnn_output_size = max_sentence_length * word_embedding_dim\n",
    "gnn_hidden_size = 64\n",
    "gnn_activation = torch.relu\n",
    "\n",
    "# 定义一个 Generator, 一个 Discriminator\n",
    "\n",
    "#G = DNN(g_input_size, g_output_size, g_hidden_size, n_hidden=n_hidden, activation=g_activation)\n",
    "#D = DNN(d_input_size, d_output_size, d_hidden_size, n_hidden=n_hidden, activation=d_activation, out_activation=torch.sigmoid)\n",
    "g_in_layers = DNN(gnn_input_size, gnn_output_size, gnn_hidden_size, n_hidden=2, activation=gnn_activation, out_activation=None)\n",
    "d_out_layers = DNN(dnn_input_size, dnn_output_size, dnn_hidden_size, n_hidden=2, activation=dnn_activation, out_activation=torch.sigmoid)\n",
    "\n",
    "G = LSTM(max_sentence_length, batch_size, word_embedding_dim, word_embedding_dim, n_hidden=1, in_layers=g_in_layers)\n",
    "D = LSTM(max_sentence_length, batch_size, word_embedding_dim, word_embedding_dim, n_hidden=1, out_layers=d_out_layers)\n",
    "DLoss = []\n",
    "GLoss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, G-loss 0.761330604553, D-loss 0.699897348881\n",
      "Thursday Thursday Luis Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Luis Thursday Thursday Thursday Luis Luis Thursday Luis Luis Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Luis\n",
      "epoch 100, G-loss 2.7599811554, D-loss 0.436667751521\n",
      "Navarro days single single single single single single single single single single single single single single single single single single January Padilla luggage luggage luggage luggage    \n",
      "epoch 200, G-loss 5.34423017502, D-loss 0.00539134198334\n",
      "Padilla January days days days Lidia days Escrig Lidia Amelia Amelia Ram'irez Alicia Padilla Alicia Ram'irez suitcase Jaime too        Ballester  Ballester Padilla\n",
      "epoch 300, G-loss 5.01646900375e-05, D-loss 0.0532207291108\n",
      "the Montero Torres twenty-ninth Lidia Montero Lidia you Lidia Ramos number Padilla Padilla Padilla leave Redondo Botella Revilla send Izquierdo Concepci'on L'opez Varela call 'Angel Rafael Ballester Padilla  \n",
      "epoch 400, G-loss 4.7507147789, D-loss 0.00574364245404\n",
      "number Elvira S'aez will  shower Botella taxi twenty-seventh Jos'e by send quarter April quarter Guijarro Gracia Izquierdo Saturday extras Ballester matter  twenty-seventh thirtieth    shower Tom'as\n",
      "epoch 500, G-loss 5.39022397995, D-loss 0.00326068961294\n",
      "suitcase Julio do Tuesday cold Susana seventh sixteen thank will by to Varela Arroyo Juan in Gracia ask bye Izquierdo we Velasco Susana sign Francisco cold form any shower Espinosa\n",
      "epoch 600, G-loss 4.00712108612, D-loss 0.00710355211049\n",
      "fourteenth Gual Mrs July leaving send Ib'a~nez Tuesday leaving Tuesday right to Wednesday which Ballester form oh Su'arez Botella looking Victoria Roda Jaime reception extras will Villanueva Calleja thirteenth Velasco\n",
      "epoch 700, G-loss 5.54323625565, D-loss 0.00226514178939\n",
      "Lobo Soler number fifteenth fifth Victoria Pastor I Sanfeli'u Jaime Fern'andez Romero car twenty-four Viciano Izquierdo Torres thirteen Lidia Rosario twelve Cabedo Cabedo eighteenth Pedro cost Villanueva Juan Villanueva Ignacio\n",
      "epoch 800, G-loss 12.3260917664, D-loss 2.40902423911e-05\n",
      "Lobo February Maestro number Gracia reception eight Julia number cold is  twenty-ninth eight   give thirteen another leaving Mar'ia Eva cold Saturday taxes  will cost problem right\n",
      "epoch 900, G-loss 5.3712515831, D-loss 0.00102474272081\n",
      "Moreno 'Oscar Ivars Roberto luggage Gumbau Roberto Montero another  Padilla  six tenth   Padilla Padilla       put     problem\n",
      "epoch 1000, G-loss 7.06923007965, D-loss 0.00063817420606\n",
      "Llorens Sunday Jes'us single Arturo Francisco town Gallego thirteenth Eva Micaela shower right cold cold travel Padilla another Mingot Eva Eva Pastor Rosa  Varela  Rosa Andr'es  Varela\n",
      "epoch 1100, G-loss 6.3445110321, D-loss 0.00117527347174\n",
      "Carmelo Sunday third luggage luggage Rosa Calleja Montero cards ninth too days right April   Padilla Padilla       twenty-seventh     problem\n",
      "epoch 1200, G-loss 9.22498512268, D-loss 3.2231843619e-05\n",
      "Llorens showing Su'arez send expensive everything taxes fifth twenty-six send Eva shower telephone Eva cold Carmelo nights another what nights the thirty-first   Varela what number Quereda five Pallar'es\n",
      "epoch 1300, G-loss 9.35662746429, D-loss 4.17749975554e-05\n",
      "Carmelo Enrique Su'arez Arturo Arturo call eighteen Roberto sign ninth  shower right send cold  Padilla tenth       Varela  Peris   \n",
      "epoch 1400, G-loss 8.69044780731, D-loss 0.000105940172944\n",
      "Llorens G'omez right Revilla evening shower twenty-four at twenty-six send Eva shower telephone quarter cold Carmelo nights my nineteen nights the Pablo Ballester cost call Velasco number Gracia five \n",
      "epoch 1500, G-loss 7.36777465704e-08, D-loss 0.000238248008827\n",
      "Llorens G'omez you very evening shower Montoro at twenty-six send cold shower Valls quarter cold Carmelo warmer  Rosal'ia Eva Eva Modesto in  call Velasco number Gracia Victoria \n",
      "epoch 1600, G-loss 1.68745604157e-08, D-loss 0.00056731906443\n",
      "key Serrano you shower Luis shower Montoro phone twenty-six quarter quarter Balaguer quarter Guijarro Eva  two Barrachina Eva Eva quarter Querol Balaguer Ramos could  nights nights  \n",
      "epoch 1700, G-loss 9.29938983917, D-loss 6.34053478734e-05\n",
      "cost Concepci'on twenty-nine shower Luis shower days make Luis ninth another Dulce cold twenty-five  Romero Gracia Barrachina  ninth shower Gallego Calatayud cards Varela  cold cold cold cost\n",
      "epoch 1800, G-loss 7.56149435043, D-loss 0.000293487207045\n",
      "cost bus  Fern'andez Ana shower Montoro Cornelles look down phone to  Ramos nights Romero Quereda Padilla    Barrachina right  Marqu'es Ballester nights   Andr'es\n",
      "epoch 1900, G-loss 18.4205799103, D-loss 0.000573113903564\n",
      "cost bus  Varela Cerezo shower days night look after Ballester Ballester could nights give Rosal'ia put Padilla give Peris  Padilla Fern'andez  Serrano Victoria cold   Peris\n",
      "epoch 2000, G-loss 18.4205799103, D-loss 9.90204653211e-05\n",
      "cost bus  Fern'andez Cerezo shower Montoro night Luis reservation Ballester Ballester could nights nights Romero cost Padilla Marqu'es Peris  Padilla Fern'andez  like  nights   \n",
      "epoch 2100, G-loss 18.4205799103, D-loss 7.25027721349e-05\n",
      "Revilla Gumbau  shower days Montoro days Montero Gumbau cold Padilla Ballester Padilla Padilla Padilla  Padilla Padilla    Padilla     Peris   \n",
      "epoch 2200, G-loss 18.4205799103, D-loss 8.05574391127e-05\n",
      "Revilla Gumbau  a days Montoro days Montero Luis Mingot Ballester Ballester Padilla nights Padilla luggage  Padilla    Padilla   like Ballester Peris   \n",
      "epoch 2300, G-loss 9.99899985032e-09, D-loss 5.98222877746e-05\n",
      "cost bus  Fern'andez Cerezo shower days night look after Ballester Ballester could nights nights Romero put Padilla give Peris  Padilla Fern'andez  Serrano Victoria nights   Peris\n",
      "epoch 2400, G-loss 18.4205799103, D-loss 5.75578642459e-05\n",
      "Piquer Enrique Padilla Luis single Lidia Lidia Montero single cold Padilla  Padilla Padilla Padilla Padilla Padilla Padilla   Padilla    cold  Padilla   \n",
      "epoch 2500, G-loss 18.4205799103, D-loss 6.39996733374e-05\n",
      "cost bus  Fern'andez Cerezo shower Montoro night Luis reservation Ballester Ballester could nights give Romero cost Padilla Marqu'es Peris  Padilla Fern'andez  like  nights   \n",
      "epoch 2600, G-loss 5.60629177094, D-loss 0.00228315478307\n",
      "cost Gumbau  could Padilla Montoro Montoro quiet Luis cold bags bags Padilla nights give Romero      Peris 'Angel  Jaime Victoria Peris   \n",
      "epoch 2700, G-loss 5.33528614044, D-loss 0.00452211441734\n",
      "Revilla Gumbau  shower single Montoro days forest checks cold Padilla Padilla Padilla Padilla Padilla too Padilla Padilla  Ballester     am Ballester Peris Padilla  \n",
      "epoch 2800, G-loss 10.8538236618, D-loss 0.00143803808078\n",
      "suitcase Torres  shower single Montoro Montoro Sergio checks am Padilla Padilla Padilla Padilla Padilla Padilla Padilla Padilla  Padilla    cold   Padilla Padilla  \n",
      "epoch 2900, G-loss 9.99899985032e-09, D-loss 0.000144414436363\n",
      "would suitcase Marqu'es twenty-nine Padilla Montoro Ib'a~nez Granada reservation  Victoria ninth look Marqu'es Pallar'es agree i  Marqu'es what   D'iaz Arturo Balaguer  Barrachina Izquierdo put Sanz\n",
      "epoch 3000, G-loss 8.21076011658, D-loss 0.000107852323936\n",
      "would suitcase Marqu'es twenty-nine Padilla Montoro room Cornelles Rosa reservation phone ninth days Ballester Padilla have twenty-fourth   Nadal   Fern'andez Arturo Balaguer  Ballester Pedro putting \n",
      "epoch 3100, G-loss 8.42822170258, D-loss 0.000154566911078\n",
      "suitcase Torres put shower shower Montoro Montoro checks checks am Peinado Padilla Padilla Padilla Padilla Padilla twenty-fourth explain  single   cold cold Ballester Ballester give Padilla  \n",
      "epoch 3200, G-loss 8.13994407654, D-loss 0.000158685541919\n",
      "would suitcase sixth Fern'andez Padilla shower room cold Rosa reservation phone ninth days Ballester V'azquez fifth oh Izquierdo  extras   Fern'andez Arturo Balaguer  D'iaz Pedro ninth \n",
      "epoch 3300, G-loss 10.2706327438, D-loss 4.038752013e-05\n",
      "would twenty-second sixth Fern'andez Escrig shower room time Rosa reservation phone phone days Ballester Nadal number April Izquierdo  extras   Marqu'es Arturo Balaguer  Ballester Pedro putting \n",
      "epoch 3400, G-loss 10.33411026, D-loss 8.5617246441e-05\n",
      "would Cantero sixth Fern'andez Cerezo shower room time Mrs reservation ninth  days Friday V'azquez Peris two Izquierdo shower extras   Fern'andez Arturo Balaguer Victoria D'iaz Pedro Ortega \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3500, G-loss 9.13814067841, D-loss 8.65606452862e-05\n",
      "Thursday Torres days shower Montoro Montoro Montoro Lidia checks am Padilla Padilla Padilla Padilla Padilla Padilla any twenty-fourth  too   cold    give Padilla Padilla luggage\n",
      "epoch 3600, G-loss 8.8805770874, D-loss 8.13872557046e-05\n",
      "would twenty-second sixth Fern'andez Padilla shower room Botella Rosa reservation Luis ninth days Ana V'azquez number form Cabo  extras    Jaime Balaguer  Ballester Pedro putting \n",
      "epoch 3700, G-loss 9.73970413208, D-loss 1.94967224236e-05\n",
      "would Rubio sixth Fern'andez Cerezo shower room Botella Mrs reservation shower  Montoro Ana V'azquez number Concepci'on Cabo a extras   week Jaime Balaguer  Ballester Pedro Peinado \n",
      "epoch 3800, G-loss 9.98909759521, D-loss 3.69602312276e-05\n",
      "would Rubio sixth Fern'andez Padilla shower room Botella Mrs reservation shower  Montoro Ana V'azquez number Concepci'on Cabo a extras   week luggage Balaguer  Ballester Pedro Peinado \n",
      "epoch 3900, G-loss 9.70452785492, D-loss 3.22071898609e-05\n",
      "would Rubio sixth Fern'andez Cerezo Su'arez Ib'a~nez Granada Mrs reservation shower  Montoro sixteenth V'azquez number Concepci'on Cabo Revilla extras    cost Balaguer  Ballester Pedro Peinado \n",
      "epoch 4000, G-loss 9.99899985032e-09, D-loss 2.67259516296e-07\n",
      "would twenty-second sixth Fern'andez Montoro Su'arez room Mr Serrano Serrano phone phone days Ballester Mestre number Concepci'on April cold extras    cost in  Ballester Pedro putting \n",
      "epoch 4100, G-loss 12.0889501572, D-loss 3.16928591815e-06\n",
      "would Rubio sixth Fern'andez Cerezo Su'arez room Montoro Mrs Monson'is twenty-second Monferrer Montoro Ana V'azquez number Concepci'on Cabo Revilla extras    Jaime Balaguer  Ballester Pedro Peinado \n",
      "epoch 4200, G-loss 9.49799919128, D-loss 3.91801220303e-05\n",
      "would Rubio sixth Balaguer Balaguer Su'arez room shower Mrs Monson'is twenty-second cost Montoro Ana V'azquez number form Cabo Revilla extras Mingot   Silvia form  D'iaz Monferrer Ortega matter\n",
      "epoch 4300, G-loss 9.21214580536, D-loss 6.27296312814e-05\n",
      "Thursday seventeen sixth Jaime Rosa Montoro Montoro Lidia checks am checks Padilla days Padilla Padilla Padilla Querol April  twenty-fourth    Romero   am Padilla  \n",
      "epoch 4400, G-loss 9.25818252563, D-loss 4.85165294535e-05\n",
      "after twenty-second October bill Rosa Su'arez room shower Serrano Manuela Ivars Friday days Ana V'azquez number form April cold extras Victoria   Silvia Balaguer  Ballester Friday putting \n",
      "epoch 4500, G-loss 10.1655073166, D-loss 1.94106311611e-05\n",
      "after twenty-second October bill Rosa Su'arez room Botella Serrano Serrano Pe~narroja Pallar'es days Ana Mestre Peris form April cold April Victoria   Silvia   Ballester Friday ninth \n",
      "epoch 4600, G-loss 10.730345726, D-loss 1.10426006898e-05\n",
      "after Rubio October bill room Su'arez room shower Pastor Monson'is twenty-second cost days Marqu'es do number form April Revilla March Victoria   Silvia Balaguer  Ballester Pedro Peinado Margarita\n",
      "epoch 4700, G-loss 11.2113924026, D-loss 7.0972830315e-06\n",
      "after twenty-second October bill room Su'arez room Botella Serrano Manuela Pe~narroja Pallar'es days Ana Carpio number form April cold April Victoria   showing Balaguer  Ballester Pedro ninth \n",
      "epoch 4800, G-loss 9.92595291138, D-loss 2.48003637253e-05\n",
      "after twenty-second October twenty-ninth room days room single Serrano Manuela Pe~narroja Pallar'es days Ana Carpio number form April cold April Victoria   showing ,  Salgado Pedro ninth \n",
      "epoch 4900, G-loss 9.04807472229, D-loss 6.27579992099e-05\n",
      "would twenty-second sixth Cabedo Balaguer days days single Cristina Serrano weeks Pallar'es days Padilla Carpio Salvador Querol April Concepci'on April    showing Jim'enez  Mingot put  \n",
      "epoch 5000, G-loss 10.0952348709, D-loss 2.09671010971e-05\n",
      "Elvira Rubio October Silvestre form days there shower Manuela Monson'is Montero Pallar'es days the Fuster Peris form March quarter conditioning Victoria   Piquer cards  i twenty-five Ortega Mar'ia\n",
      "epoch 5100, G-loss 10.5017766953, D-loss 1.38984391533e-05\n",
      "R'ios twenty-second twenty-second Cabedo form days Manuela phone Cristina am weeks checks days Padilla Padilla Padilla Querol April Concepci'on April    cards cards  Luis   \n",
      "epoch 5200, G-loss 10.6914424896, D-loss 1.134830514e-05\n",
      "Elvira Rubio October today sorry days Mrs expenses give taking Montero checks days the down Peris Arturo March quarter conditioning Victoria   Piquer cards a i twenty-five Ortega Mar'ia\n",
      "epoch 5300, G-loss 10.7541208267, D-loss 1.10064260817e-05\n",
      "Elvira Rubio October today sorry days Mrs expenses give moving Montero Pallar'es days the Juan Peris Arturo March quarter Marta Victoria   Piquer Rosal'ia a i twenty-five  Mar'ia\n",
      "epoch 5400, G-loss 9.28058719635, D-loss 6.03345283317e-05\n",
      "R'ios twenty-second twenty-second Jim'enez Concepci'on days Arnau Montero weeks Virginia weeks Padilla days Padilla Carpio Victoria room April Gracia April    cards cards  Luis   shower\n",
      "epoch 5500, G-loss 9.47838783264, D-loss 6.15352759787e-05\n",
      "Elvira Cantero Gonz'alez Jim'enez Concepci'on Arroyo expensive carry with Jorge Paloma April Paloma Izquierdo Paloma five Arturo four Gracia R'ios Balaguer   o'clock send a Miralles put  \n",
      "epoch 5600, G-loss 11.0558004379, D-loss 1.11043846118e-05\n",
      "April Cantero Villanueva twenty-ninth like Arroyo expensive Pallar'es with Viciano April April Espinosa April Varela  D'iaz V'azquez Barrachina Gracia   like Concepci'on  Jim'enez !   what\n",
      "epoch 5700, G-loss 5.80684757233, D-loss 0.00228312527712\n",
      "car lot Villanueva twenty-ninth like Querol Arnau tomorrow July moving April Eva two any evening  waking D'iaz Gloria Marina Ramos what am Saturday Calatayud Jim'enez ! put  \n",
      "epoch 5800, G-loss 18.4205799103, D-loss 6.46036508734e-08\n",
      "car lot fourteen Santos like Amelia Arnau waking July moving April Eva two twenty-one down  waking D'iaz Gloria Marina Ramos what Marina Saturday Calatayud Jim'enez ! put  \n",
      "epoch 5900, G-loss 13.7353591919, D-loss 7.8155098393e-07\n",
      "Bordons lot Piquer Jim'enez reservation Ram'irez Arnau Bellver Pastor Viciano April April Joaqu'in Susana Arenas  waking price twenty-seventh Marina  what leaving Saturday Marina  Rivera   Ramos\n",
      "epoch 6000, G-loss 8.50468349457, D-loss 0.00500609800838\n",
      "Manuel for Revilla quiet Mrs Serrano November Bordons extras Viciano April Eva where June Jorge Marqu'es price price form Gracia  twenty-two leaving Saturday twenty-five Dulce July   \n",
      "epoch 6100, G-loss 11.1990222931, D-loss 4.08281852771e-07\n",
      "Manuel Gual will Miralles Mrs Balaguer November Pallar'es send Viciano April April Joaqu'in R'ios Viciano  D'iaz price Barrachina Gracia  twenty-two leaving in cost am July   Concepci'on\n",
      "epoch 6200, G-loss 10.6273918152, D-loss 2.65147878808e-06\n",
      "Manuel fifteen Piquer Rosal'ia Arturo Balaguer November Pallar'es with Mart'i April Eva Joaqu'in Cabo Cabo  D'iaz price Barrachina Gracia   Concepci'on Concepci'on cost  luggage   Concepci'on\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-7b07a0379801>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0md_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_real_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0md_real_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0md_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/torch/tensor.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m \u001b[0;34m`\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/torch/autograd/__init__.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 模拟参数设置\n",
    "\n",
    "lr = 1e-3\n",
    "\n",
    "data_size = len(target_sentences)\n",
    "n_epochs = 50000\n",
    "\n",
    "d_steps = 1\n",
    "g_steps = 1\n",
    "\n",
    "\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "g_optimizer = torch.optim.Adam(G.parameters(),lr=lr)\n",
    "d_optimizer = torch.optim.Adam(D.parameters(),lr=lr)\n",
    "\n",
    "def lr_lambda(epoch):\n",
    "    return 0.99**(epoch/50.0)\n",
    "\n",
    "g_scheduler = torch.optim.lr_scheduler.LambdaLR(g_optimizer, lr_lambda, last_epoch=-1)\n",
    "d_scheduler = torch.optim.lr_scheduler.LambdaLR(d_optimizer, lr_lambda, last_epoch=-1)\n",
    "\n",
    "criterion = nn.BCELoss() #torch.nn.SmoothL1Loss() # or torch.nn.MSELoss()\n",
    "\n",
    "display_interval = 100\n",
    "\n",
    "# 模型优化\n",
    "#loser = 'D'\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    d_loss = []\n",
    "    g_loss = []\n",
    "    \n",
    "    for d in range(d_steps):\n",
    "        \n",
    "        # 归零D的梯度\n",
    "        G.zero_grad()\n",
    "        D.zero_grad()\n",
    "        \n",
    "        # 生成一个 batch 的真实样本\n",
    "        indices = np.random.randint(0, data_size, batch_size)\n",
    "        real_samples = get_sentence_embedding(target_sentences[indices[0]], mode='target')\n",
    "        for i in range(1, batch_size):\n",
    "            real_samples = torch.cat((real_samples, get_sentence_embedding(target_sentences[indices[i]], mode='target')))\n",
    "        real_samples = real_samples.detach()\n",
    "        d_real_err = criterion(D(real_samples),torch.ones(batch_size, 1))\n",
    "        d_loss.append(d_real_err.tolist())\n",
    "        \n",
    "        d_real_err.backward()\n",
    "        d_optimizer.step()\n",
    "\n",
    "        # 归零D的梯度\n",
    "        G.zero_grad()\n",
    "        D.zero_grad()\n",
    "        \n",
    "        # 生成一个 batch 的虚假样本\n",
    "        if np.random.random()<0.9:\n",
    "            fake_samples = G(torch.rand(batch_size, gnn_input_size))\n",
    "        else:\n",
    "            fake_samples = torch.randn(batch_size, max_sentence_length, word_embedding_dim)\n",
    "        \n",
    "        d_fake_err = criterion(D(fake_samples),torch.zeros(batch_size, 1))\n",
    "        d_loss.append(d_fake_err.tolist())\n",
    "        \n",
    "        d_fake_err.backward()\n",
    "        d_optimizer.step()\n",
    "        d_scheduler.step()\n",
    "        \n",
    "    for g in range(g_steps):\n",
    "        \n",
    "        G.zero_grad()\n",
    "        D.zero_grad()\n",
    "        if np.random.random()<0.9:\n",
    "            fake_samples = G(torch.rand(batch_size, gnn_input_size))\n",
    "            g_err = criterion(D(fake_samples),torch.ones(batch_size, 1))\n",
    "        else:\n",
    "            fake_samples = G(-torch.rand(batch_size, gnn_input_size))\n",
    "            g_err = criterion(D(fake_samples),torch.zeros(batch_size, 1))\n",
    "        g_loss.append(g_err.tolist())\n",
    "        \n",
    "        g_err.backward()\n",
    "        g_optimizer.step()\n",
    "        g_scheduler.step()\n",
    "        \n",
    "    if len(d_loss)>0:\n",
    "        DLoss.append(np.mean(d_loss))\n",
    "    elif len(DLoss)>0:\n",
    "        DLoss.append(DLoss[-1])\n",
    "    else:\n",
    "        DLoss.append(np.inf)\n",
    "    if len(g_loss)>0:\n",
    "        GLoss.append(np.mean(g_loss))\n",
    "    elif len(GLoss)>0:\n",
    "        GLoss.append(GLoss[-1])\n",
    "    else:\n",
    "        GLoss.append(np.inf)\n",
    "        \n",
    "    if epoch % display_interval == 0:\n",
    "        print(r'epoch {}, G-loss {}, D-loss {}'.format(epoch, GLoss[-1], DLoss[-1]))\n",
    "        print(target_sentence_embedding2words(G(torch.randn(1, gnn_input_size)).view(-1).detach()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 50\n",
    "avg_losses = np.array(DLoss)[:len(DLoss)//50 *50].reshape([-1,window_size]).mean(1)\n",
    "pl.plot(np.arange(0,len(DLoss)//50 *50,window_size), avg_losses,'r-',label='D-loss')\n",
    "avg_losses = np.array(GLoss)[:len(GLoss)//50 *50].reshape([-1,window_size]).mean(1)\n",
    "pl.plot(np.arange(0,len(GLoss)//50 *50,window_size), avg_losses,'g--',label='G-loss')\n",
    "pl.legend(loc='best')\n",
    "pl.xlabel('Time')\n",
    "pl.ylabel('Loss')\n",
    "pl.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_samples.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_embeddings_target.weight[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_samples.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion(torch.zeros(5,1).clamp(1e-8,1-1e-7),torch.zeros(5,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.imshow(D.parameters().next().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D(fake_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nan<np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings_target.weight[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

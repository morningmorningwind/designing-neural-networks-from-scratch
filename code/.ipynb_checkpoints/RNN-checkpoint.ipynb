{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入模块\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "from torch import nn\n",
    "\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个由多层线性层构成的的简单RNN单元\n",
    "class ERNN_Cell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, activation=None):\n",
    "        super(ERNN_Cell,self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.activation = activation\n",
    "        self.input_x = nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.input_h = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        x = self.input_x(x)\n",
    "        h = self.input_h(h)\n",
    "        if activation is not None:\n",
    "            return activation(x+h)\n",
    "        else:\n",
    "            return x + h\n",
    "\n",
    "# 定义一个多层的简单RNN\n",
    "class ERNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, activation = None):\n",
    "        super(ERNN,self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.activation = activation\n",
    "        self.ernn = ERNN_Cell(self.input_size, self.hidden_size, activation=self.activation)\n",
    "    def forward(self, x, h_t=None):\n",
    "        self.seq_length, self.batch_size, self.input_size = x.size()\n",
    "        ys = []\n",
    "        if h_t is None:\n",
    "            h_t = torch.zeros(1,1,self.hidden_size)\n",
    "        for t in range(0, self.seq_length):\n",
    "            h_t = self.ernn(x[t], h_t)\n",
    "            ys.append(h_t.view(1,1,-1))\n",
    "        self.ys = torch.cat(ys)\n",
    "        return self.ys, h_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data summary:\n",
      "\n",
      " number of poems: 10184\n",
      " number of words: 7650\n",
      "\n",
      "Poem examples:\n",
      "\n",
      "S于太原召侍臣赐宴守岁:四时运灰琯，一夕变冬春。送寒余雪尽，迎岁早梅新。E\n",
      "S咏弓:上弦明月半，激箭流星远。落雁带书惊，啼猿映枝转。E\n",
      "S赋得早雁出云鸣:初秋玉露清，早雁出空鸣。隔云时乱影，因风乍含声。E\n",
      "S赋得临池柳:岸曲丝阴聚，波移带影疏。还将眉里翠，来就镜中舒。E\n",
      "S赋得临池竹:贞条障曲砌，翠叶贯寒霜。拂牖分龙影，临池待凤翔。E\n",
      "S赋得弱柳鸣秋蝉:散影玉阶柳，含翠隐鸣蝉。微形藏叶里，乱响出风前。E\n",
      "S探得李:盘根直盈渚，交干横倚天。舒华光四海，卷叶荫三川。E\n",
      "S咏小山:近谷交萦蕊，遥峰对出莲。径细无全磴，松小未含烟。E\n",
      "S赐萧瑀:疾风知劲草，板荡识诚臣。勇夫安识义，智者必怀仁。E\n",
      "S赐房玄龄:太液仙舟迥，西园隐上才。未晓征车度，鸡鸣关早开。E\n"
     ]
    }
   ],
   "source": [
    "# 训练一个基于ERNN神经网络来作诗\n",
    "\n",
    "## 读入用GloVe处理得到的文字 embeddings，以及句子数据。\n",
    "import codecs\n",
    "\n",
    "with codecs.open('data/word_embeddings.txt', mode='r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "n_words = len(lines)\n",
    "word_emb_dim = input_size = 20\n",
    "word_embeddings = torch.nn.Embedding(n_words, word_emb_dim)\n",
    "i2w = {}\n",
    "w2i = {}\n",
    "for i in range(n_words):\n",
    "    line = lines[i].split(' ')\n",
    "    i2w[i] = line[0]\n",
    "    w2i[line[0]] = i\n",
    "    word_embeddings.weight[i] = torch.from_numpy(np.array(line[1:],dtype=np.float32))\n",
    "\n",
    "word_embeddings.weight.require_grad = False\n",
    "\n",
    "poems = []\n",
    "max_line_length = 50\n",
    "with codecs.open('data/poems.txt', mode='r', encoding='utf-8') as f:\n",
    "    for poem in f:\n",
    "        poem = poem.replace(' ','')\n",
    "        poem = poem.replace('\\n','')\n",
    "        if len(poem) < 10 or len(poem) > max_line_length or '(' in poem or u'（' in poem or u'）' in poem or ')' in poem or '_' in poem or ':' not in poem:\n",
    "            continue\n",
    "        poem = 'S'+poem+'E'\n",
    "        poems.append(poem)\n",
    "\n",
    "n_poems = len(poems)\n",
    "\n",
    "print( 'Data summary:\\n\\n number of poems: {}\\n number of words: {}\\n'.format(n_poems, n_words))\n",
    "print('Poem examples:\\n\\n'+'\\n'.join(poems[:10]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S发硖石路上却寄内:莎栅东行五谷深，千峰万壑雨沈沈。细君几日路经此，应见悲翁相望心。\n",
      "发硖石路上却寄内:莎栅东行五谷深，千峰万壑雨沈沈。细君几日路经此，应见悲翁相望心。E\n"
     ]
    }
   ],
   "source": [
    "# 定义一个函数，随机返回一个 mini batch，用于训练，由于每一首诗歌的长度不同，我们此处规定每个batch只有一首诗。这样，就可以生成长度可变的诗歌。\n",
    "def get_batch(batch_size=1):\n",
    "    batch= []\n",
    "    for i in range(batch_size):\n",
    "        poem = map(w2i.get, poems[np.random.randint(0,n_poems)])\n",
    "        batch.append(poem)\n",
    "    batch = torch.LongTensor(batch).t().view(-1,batch_size,1)\n",
    "    x = word_embeddings(batch[:-1]).detach().squeeze(2)\n",
    "    x.requires_grad=False\n",
    "    y = batch[1:]\n",
    "    return x, y\n",
    "\n",
    "def get_batch(batch_size=1):\n",
    "    batch= []\n",
    "    for i in range(batch_size):\n",
    "        poem = map(w2i.get, poems[np.random.randint(0,n_poems)])\n",
    "        batch.append(poem)\n",
    "    batch = torch.LongTensor(batch).t().view(-1,batch_size,1)\n",
    "    x = word_embeddings(batch[:-1]).detach().squeeze(2)\n",
    "    y = word_embeddings(batch[1:]).detach().squeeze(2)\n",
    "    return x, y\n",
    "\n",
    "# 定义一个函数，输入一个 sentence embedding 返回一个句子\n",
    "def emb2sent(emb):\n",
    "    seq_length, batch_size, emb_size = emb.size()\n",
    "    s = []\n",
    "    for i in range(batch_size):\n",
    "        for j in range(seq_length):\n",
    "            n = torch.argmin(torch.mean((word_embeddings.weight - emb[j,i,:])**2,1)).tolist()\n",
    "            s.append(i2w[n])\n",
    "    return s\n",
    "\n",
    "x, y = get_batch()\n",
    "print(''.join(emb2sent(x)))\n",
    "print(''.join(emb2sent(y)))\n",
    "\n",
    "# 定义一个函数，输入一个汉字，生成一首诗\n",
    "def poem_gen(model, w=None):\n",
    "    if not w in w2i or w is None:\n",
    "        print(u'Warning: {} is not in the dictionary, random initial word is selected.'.format(w))\n",
    "        idx = np.random.randint(0,n_words)\n",
    "        w = i2w[idx]\n",
    "    else:\n",
    "        idx = w2i[w]\n",
    "    \n",
    "    y_t, h_t = model(word_embeddings.weight[w2i['S']].view(1,1,-1))\n",
    "    x_t = word_embeddings.weight[idx].view(1,1,-1)\n",
    "    s = []\n",
    "    s.append(w)\n",
    "    for t in range(max_line_length):\n",
    "        y_t, h_t = model(x_t, h_t)\n",
    "        idx = torch.argmax(y_t.view(-1)).tolist()\n",
    "        x_t = word_embeddings.weight[idx].view(1,1,-1)\n",
    "        w = i2w[idx]\n",
    "        s.append(w)\n",
    "        if w == 'E':\n",
    "            break\n",
    "    return u''.join(s)\n",
    "\n",
    "def poem_gen(model, w=None):\n",
    "    if not w in w2i or w is None:\n",
    "        print(u'Warning: {} is not in the dictionary, random initial word is selected.'.format(w))\n",
    "        idx = np.random.randint(0,n_words)\n",
    "        w = i2w[idx]\n",
    "    else:\n",
    "        idx = w2i[w]\n",
    "    \n",
    "    y_t, h_t = model(word_embeddings.weight[w2i['S']].view(1,1,-1))\n",
    "    x_t = word_embeddings.weight[idx].view(1,1,-1)\n",
    "    s = []\n",
    "    s.append(w)\n",
    "    for t in range(max_line_length):\n",
    "        x_t, h_t = model(x_t, h_t)\n",
    "        w = emb2sent(x_t)[0]\n",
    "        s.append(w)\n",
    "        if w == 'E':\n",
    "            break\n",
    "    return u''.join(s)\n",
    "\n",
    "# 定义一个生成器\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size, activation=torch.tanh):\n",
    "        super(Generator, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.activation = activation\n",
    "        self.ernn = nn.LSTM(self.input_size, self.hidden_size,2)#ERNN(self.input_size, self.hidden_size, activation=self.activation)\n",
    "        self.output = nn.Linear(self.hidden_size,self.output_size)\n",
    "        self.softmax = torch.nn.Softmax(-1)\n",
    "    def forward(self, x_t, h_t=None):     \n",
    "        self.seq_length, self.batch_size, self.input_size = x_t.size()\n",
    "        ys, h_t = self.ernn(x_t,h_t)\n",
    "        ys = self.softmax(self.output(ys))\n",
    "        return ys, h_t\n",
    "\n",
    "# 训练一个简单的 RNN 模型以生成诗歌\n",
    "\n",
    "lr = 1e-3\n",
    "\n",
    "input_size = 20\n",
    "hidden_size = 128\n",
    "output_size = input_size\n",
    "activation = torch.relu\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self,input_size, hidden_size, output_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.input = nn.LSTM(input_size, hidden_size)\n",
    "        self.hidden = nn.LSTM(hidden_size, hidden_size)\n",
    "        self.output = nn.LSTM(hidden_size, output_size)\n",
    "    def forward(self,x, ht=None):\n",
    "        x, ht0 = self.input(x, ht)\n",
    "        x, ht = self.hidden(x)\n",
    "        x, ht = self.output(x)\n",
    "        return x, ht0\n",
    "\n",
    "model = nn.LSTM(input_size, output_size, num_layers=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: None is not in the dictionary, random initial word is selected.\n",
      "Epoch 0, Loss 0.276147037745\n",
      " 荤并□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "Warning: None is not in the dictionary, random initial word is selected.\n",
      "Epoch 200, Loss 0.147929400206\n",
      " 妆并并并当当当长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长\n",
      "Warning: None is not in the dictionary, random initial word is selected.\n",
      "Epoch 400, Loss 0.151375249028\n",
      " 湖当长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长\n",
      "Warning: None is not in the dictionary, random initial word is selected.\n",
      "Epoch 600, Loss 0.161787003279\n",
      " 具并正当长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长\n",
      "Warning: None is not in the dictionary, random initial word is selected.\n",
      "Epoch 800, Loss 0.180221825838\n",
      " 簖并并并当当长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长\n",
      "Warning: None is not in the dictionary, random initial word is selected.\n",
      "Epoch 1000, Loss 0.109780102968\n",
      " 绮正正长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长\n",
      "Warning: None is not in the dictionary, random initial word is selected.\n",
      "Epoch 1200, Loss 0.109579712152\n",
      " 虚曾当长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长\n",
      "Warning: None is not in the dictionary, random initial word is selected.\n",
      "Epoch 1400, Loss 0.124667845666\n",
      " 髟并余当当长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长\n",
      "Warning: None is not in the dictionary, random initial word is selected.\n",
      "Epoch 1600, Loss 0.122702553868\n",
      " 驴并当当长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长\n",
      "Warning: None is not in the dictionary, random initial word is selected.\n",
      "Epoch 1800, Loss 0.115903012455\n",
      " 逑并并正长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长\n",
      "Warning: None is not in the dictionary, random initial word is selected.\n",
      "Epoch 2000, Loss 0.146326094866\n",
      " 柬并并中中长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长\n",
      "Warning: None is not in the dictionary, random initial word is selected.\n",
      "Epoch 2200, Loss 0.178173154593\n",
      " 柬并并中中长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长\n",
      "Warning: None is not in the dictionary, random initial word is selected.\n",
      "Epoch 2400, Loss 0.123095408082\n",
      " 萸并当长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长\n",
      "Warning: None is not in the dictionary, random initial word is selected.\n",
      "Epoch 2600, Loss 0.13979922235\n",
      " 溉余当当长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长\n",
      "Warning: None is not in the dictionary, random initial word is selected.\n",
      "Epoch 2800, Loss 0.128678634763\n",
      " 尚尚中中中长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长\n",
      "Warning: None is not in the dictionary, random initial word is selected.\n",
      "Epoch 3000, Loss 0.137013658881\n",
      " 猴并当长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长\n",
      "Warning: None is not in the dictionary, random initial word is selected.\n",
      "Epoch 3200, Loss 0.152002364397\n",
      " 穉并并当长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长\n",
      "Warning: None is not in the dictionary, random initial word is selected.\n",
      "Epoch 3400, Loss 0.154250919819\n",
      " ─并并中中长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长\n",
      "Warning: None is not in the dictionary, random initial word is selected.\n",
      "Epoch 3600, Loss 0.139636874199\n",
      " 遏并正长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长\n",
      "Warning: None is not in the dictionary, random initial word is selected.\n",
      "Epoch 3800, Loss 0.13781017065\n",
      " 蹎尚尚长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长\n",
      "Warning: None is not in the dictionary, random initial word is selected.\n",
      "Epoch 4000, Loss 0.142597332597\n",
      " 蠋并曾长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长\n",
      "Warning: None is not in the dictionary, random initial word is selected.\n",
      "Epoch 4200, Loss 0.147340834141\n",
      " 项尚中中中中长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长\n",
      "Warning: None is not in the dictionary, random initial word is selected.\n",
      "Epoch 4400, Loss 0.161976620555\n",
      " 掉并长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长\n",
      "Warning: None is not in the dictionary, random initial word is selected.\n",
      "Epoch 4600, Loss 0.126867681742\n",
      " 窆并入入长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长\n",
      "Warning: None is not in the dictionary, random initial word is selected.\n",
      "Epoch 4800, Loss 0.163483321667\n",
      " 门中中长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长\n",
      "Warning: None is not in the dictionary, random initial word is selected.\n",
      "Epoch 5000, Loss 0.218238443136\n",
      " 獍并曾中中长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长\n",
      "Warning: None is not in the dictionary, random initial word is selected.\n",
      "Epoch 5200, Loss 0.118816874921\n",
      " 觚并尚当长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长\n",
      "Warning: None is not in the dictionary, random initial word is selected.\n",
      "Epoch 5400, Loss 0.123361386359\n",
      " 劘并当长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长\n",
      "Warning: None is not in the dictionary, random initial word is selected.\n",
      "Epoch 5600, Loss 0.120105601847\n",
      " 哑梁正长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长\n",
      "Warning: None is not in the dictionary, random initial word is selected.\n",
      "Epoch 5800, Loss 0.107391558588\n",
      " 黮并曾见长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长长\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-247-d80f7cf8eb05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0myt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mht\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mLoss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/torch/tensor.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m \u001b[0;34m`\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/torch/autograd/__init__.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "n_epochs = 10000\n",
    "last_epoch = -1\n",
    "disp_interval = 200\n",
    "\n",
    "loss_func = nn.SmoothL1Loss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n",
    "\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "def lr_lambda(epoch):\n",
    "    return 0.99**(epoch/50.0)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch=last_epoch)\n",
    "\n",
    "Loss = []\n",
    "for epoch in range(n_epochs):\n",
    "    x = []\n",
    "    y = []\n",
    "    x, y = get_batch()\n",
    "    model.zero_grad()\n",
    "    yt,ht = model(x)\n",
    "    loss = loss_func(yt,y)\n",
    "    loss.backward()\n",
    "    Loss.append(loss.tolist())\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    if epoch % disp_interval == 0:\n",
    "        print(u'Epoch {}, Loss {}\\n {}'.format(epoch,loss.tolist(), poem_gen(model)))\n",
    "        \n",
    "        \n",
    "window_size = 50\n",
    "avg_losses = np.array(Loss)[:len(Loss)//50 *50].reshape([-1,window_size]).mean(1)\n",
    "pl.plot(np.arange(0,len(Loss)//50 *50,window_size), avg_losses,'r-')\n",
    "pl.xlabel('Time')\n",
    "pl.ylabel('Loss')\n",
    "pl.yscale('log')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1807,  0.6392,  0.1268,  1.1103, -1.6153, -0.1511, -0.0435,\n",
       "        -0.0471, -0.8821, -0.4894, -0.1626,  0.1210,  1.6830, -0.1591,\n",
       "        -0.0498, -0.0988,  0.3077,  1.0061,  0.2484, -0.7002])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embeddings.weight[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([41, 1, 20])"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([41, 1, 7650])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func(yt,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([41, 1, 20])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embeddings(torch.argmax(yt,-1)).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5192)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(yt[0,0,:].view(1,1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7650])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yt[0,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 1, 20])"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

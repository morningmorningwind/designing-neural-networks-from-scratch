{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入模块\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "from torch import nn\n",
    "import re\n",
    "\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data summary:\n",
      "\n",
      " number of poems: 220\n",
      " number of words: 1756\n",
      "\n",
      "Poem examples:\n",
      "\n",
      "长空雁，霜晨月。投笔怀牧之，飞书笑鲁连。红尘踏马惯烟雪，孤村坐卧赏寒绝。E\n",
      "寄琴剑，托凤笺。素肴酬知己，行杯梦真玄。水卢文艺遣高情，古今风物共流年。E\n",
      "平生何所寄？天地一孤篷。郁纡且行游，迟复尘景中。E\n",
      "星汉奔岩屿，惊涛卷曈虹。翕趿隐烟色，长桥海岛空。E\n",
      "百年如云梦，逆旅何匆匆。吟坐忘知闻，拈花鉴溟濛。E\n",
      "道心不外求，日影养虚冲。观风遣剑意，抱朴任穷通。E\n",
      "千古一杯清，卧剑亦何如？云雁有芳信，谈笑未成书。E\n",
      "故国弛山色，春华因才逐。北庭惜玉折，积风待岁除。E\n",
      "俯仰苍茫间，太虚应有诸。值此吟月夜，借居怀纡余。E\n",
      "心斋即坛醮，守道安违俗。江湖得意气，狂歌岂踟躇。E\n"
     ]
    }
   ],
   "source": [
    "# 训练一个基于ERNN神经网络来作诗\n",
    "\n",
    "## 读入用GloVe处理得到的文字 embeddings，以及句子数据。\n",
    "import codecs\n",
    "\n",
    "word_emb_dim = input_size = 1\n",
    "i2w = {0:''}\n",
    "w2i = {'':0}\n",
    "\n",
    "word_emb_dim = 128\n",
    "\n",
    "with codecs.open('data/word_embeddings_manyun-mix_128.txt', mode='r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    n_words = len(lines)+1\n",
    "    word_embeddings = torch.nn.Embedding(n_words, word_emb_dim)\n",
    "    for i in range(1, n_words):\n",
    "        line = lines[i-1].split(' ')\n",
    "        i2w[i] = line[0]\n",
    "        w2i[line[0]] = i\n",
    "        word_embeddings.weight[i] = torch.from_numpy(np.array(line[1:],dtype=np.float32))\n",
    "\n",
    "max_line_length = 50\n",
    "poems = []\n",
    "with codecs.open('data/manyun-mix.txt', mode='r', encoding='utf-8') as f:\n",
    "    for poem in f:\n",
    "        poem = re.sub('\\s','',poem)\n",
    "        if ':' in poem:\n",
    "            poem = poem.split(':')[-1]\n",
    "        poem = 'S'+poem+'E'\n",
    "        if len(poem) < 10 or len(poem) > max_line_length or '(' in poem or u'（' in poem or u'《' in poem or '-' in poem or '_' in poem:\n",
    "            continue\n",
    "#        poem = re.split(u'[。；.?？]', poem)\n",
    "#        for s in poem:\n",
    "#            if len(s)>3:\n",
    "#                s += u'。'\n",
    "        poems.append(map(w2i.get, poem))\n",
    "\n",
    "n_poems = len(poems)\n",
    "\n",
    "print( 'Data summary:\\n\\n number of poems: {}\\n number of words: {}\\n'.format(n_poems, n_words))\n",
    "print('Poem examples:\\n\\n'+'\\n'.join([''.join(map(i2w.get, x)) for x in poems[:10]]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "从来湖上胜人间，远爱浮云独自还。孤月空天见心地，寥寥一水镜中山。\n",
      "来湖上胜人间，远爱浮云独自还。孤月空天见心地，寥寥一水镜中山。E\n"
     ]
    }
   ],
   "source": [
    "# 定义一个函数，随机返回一个 mini batch，用于训练，由于每一首诗歌的长度不同，我们此处规定每个batch只有一首诗。这样，就可以生成长度可变的诗歌。\n",
    "def get_batch(batch_size=2):\n",
    "    batch_raw = [poems[i][:] for i in np.random.randint(0, n_poems, batch_size)]\n",
    "    max_length = max(map(len, batch_raw))\n",
    "    for i in range(len(batch_raw)):\n",
    "        for j in range(len(batch_raw[i]),max_length):\n",
    "            batch_raw[i].append(0)\n",
    "    batch_raw = torch.LongTensor(batch_raw).detach().unsqueeze(2).transpose(0,1)\n",
    "    x = batch_raw[:-1].type(torch.float32)\n",
    "    y = batch_raw[1:]\n",
    "    return x, y\n",
    "\n",
    "def idx2emb(x):\n",
    "    return word_embeddings(x.type(torch.long)).squeeze(2).detach()\n",
    "    \n",
    "\n",
    "# 定义一个函数，输入一个 batch 返回句子\n",
    "def batch2sent(batch):\n",
    "    S = []\n",
    "    batch = batch.type(torch.int32).detach()\n",
    "    seq_length, batch_size, emb_size = batch.size()\n",
    "    for i in range(batch_size):\n",
    "        S.append(''.join(map(i2w.get, batch[:,i,:].view(-1).tolist())))\n",
    "    return u'\\n'.join(S)\n",
    "\n",
    "x, y = get_batch(1)\n",
    "print(batch2sent(x))\n",
    "print(batch2sent(y))\n",
    "\n",
    "# 定义一个生成器\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size, n_layers=2, activation=None):\n",
    "        super(Generator, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.activation = activation\n",
    "        self.rnn = nn.LSTM(self.input_size, self.hidden_size, num_layers=self.n_layers, dropout=0.01)\n",
    "        self.output = nn.Linear(self.hidden_size,self.output_size)\n",
    "        self.softmax = torch.nn.LogSoftmax(dim=-1)\n",
    "    def init_h(self):\n",
    "        return (torch.randn(self.n_layers, self.batch_size, self.hidden_size),torch.randn(self.n_layers, self.batch_size, self.hidden_size))\n",
    "    def forward(self, x, h0=None):\n",
    "        self.seq_length, self.batch_size, self.input_size = x.size()\n",
    "        if h0 is None:\n",
    "            h0 = self.init_h()\n",
    "#            x0 = torch.FloatTensor([w2i['S']]).view(1,1,-1).detach()\n",
    "#            x0 = idx2emb(x0)\n",
    "#            y0, h0 = self.rnn(x0,h0)\n",
    "        y, ht = self.rnn(x,h0)\n",
    "#        y = torch.cat((y0,y),dim=0)\n",
    "        y = y.view(-1,self.hidden_size)\n",
    "        y = self.output(y)\n",
    "        y = y.view(self.seq_length,self.batch_size,self.output_size)\n",
    "        y = self.softmax(y)\n",
    "        return y, ht\n",
    "\n",
    "def poem_gen(model, w=None, cr=1e-1):\n",
    "    with torch.no_grad():\n",
    "        if not w in w2i or w is None:\n",
    "            idx = np.random.randint(1,n_words)\n",
    "            w = i2w[idx]\n",
    "        else:\n",
    "            idx = w2i[w]\n",
    "        ht = None\n",
    "        x0 = torch.FloatTensor([w2i['S']]).view(1,1,-1).detach()\n",
    "        x0 = idx2emb(x0)\n",
    "        y, ht = model(x0, ht)\n",
    "        x = torch.LongTensor([w2i[w]]).view(1,1,-1).detach()\n",
    "        x = idx2emb(x)\n",
    "        s = []\n",
    "        s.append(w)\n",
    "        for t in range(max_line_length):\n",
    "            y, ht = model(x, ht)\n",
    "            not_done = True\n",
    "            cnt = 0\n",
    "            while not_done and cnt <50:\n",
    "                k = min([1+np.random.binomial(3,0.5), y.size(-1)-1])\n",
    "                x = torch.topk(y, k, dim=-1)[1].detach()\n",
    "                x = x[:,:,min([np.random.geometric(0.3), k-1])].unsqueeze(2)\n",
    "#                x = torch.argmax(y,dim=-1,keepdim=True)\n",
    "                cnt += 1\n",
    "                w = batch2sent(x)\n",
    "                if s.count(w)<1: not_done = False\n",
    "            if w == 'E':\n",
    "                break\n",
    "            s.append(w)\n",
    "            x = idx2emb(x)\n",
    "        return u''.join(s)\n",
    "    \n",
    "    \n",
    "# 训练一个简单的 RNN 模型以生成诗歌\n",
    "\n",
    "input_size = word_emb_dim\n",
    "hidden_size = 128\n",
    "output_size = n_words\n",
    "activation = torch.relu\n",
    "\n",
    "model = Generator(input_size, output_size, hidden_size, n_layers=2, activation=activation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch0, Loss0.0010306944605\n",
      "Pred:\n",
      "翩公子，在陌之桑。剑胆琴心，气宇轩昂。E\n",
      "Obs:\n",
      "翩公子，在陌之桑。剑胆琴心，气宇轩昂。E\n",
      "Rnd:\n",
      "信颠踣故今无过，禅心正欲甚慰天冷。况一春此忧尽人地人\n",
      "\n",
      "Epoch50, Loss0.0782466679811\n",
      "Pred:\n",
      "阳台昙花三五成蕾，似欲娇绽，余心甚慰，常与之对坐倾谈。E\n",
      "Obs:\n",
      "阳台昙花三五成蕾，似欲娇绽，余心甚慰，常与之对坐倾谈。E\n",
      "Rnd:\n",
      "婺小削奇秀人秋思意，禅资去时迟云北子归来月。知堪有欢见尽心\n",
      "\n",
      "Epoch100, Loss0.175071612\n",
      "Pred:\n",
      "假独拥酒醒，纵有兴、怕逞余杯。E\n",
      "Obs:\n",
      "竟独拥酒醒，纵有兴、怕逞余杯。E\n",
      "Rnd:\n",
      "生分已在心名，今日无我须放。一朝天柳色落客雪客人\n",
      "\n",
      "Epoch150, Loss0.100216180086\n",
      "Pred:\n",
      "甚少在人群之中提及妳E\n",
      "Obs:\n",
      "甚少在人群之中提及妳E\n",
      "Rnd:\n",
      "鲁颠行人路远寒。山情海、秋归，芳我亦天无中苑云湘枝长谁忆冷尽。\n",
      "\n",
      "Epoch200, Loss0.206890508533\n",
      "Pred:\n",
      "日趣不已，东风吹绿蘋。欲看梅市雪，知赏柳家春。别意倾吴醑，芳声动越人。山阴三月会，内史得嘉宾。E\n",
      "Obs:\n",
      "舸趣不已，东风吹绿蘋。欲看梅市雪，知赏柳家春。别意倾吴醑，芳声动越人。山阴三月会，内史得嘉宾。E\n",
      "Rnd:\n",
      "共风子出人在心，细舟今留家石情\n",
      "\n",
      "Epoch250, Loss0.228725373745\n",
      "Pred:\n",
      "欲性情奇，初生玉树枝。人曾天上见，名向月中知。我识婴儿意，何须待佩觿。E\n",
      "Obs:\n",
      "子性情奇，初生玉树枝。人曾天上见，名向月中知。我识婴儿意，何须待佩觿。E\n",
      "Rnd:\n",
      "边马后生，秋古一何天行云空烟枝落。离上摇子应远情心名知我与柏年\n",
      "\n",
      "Epoch300, Loss0.570518374443\n",
      "Pred:\n",
      "吹未长已暮雨难将息。叶底揽花痕，并蒂瘦、娇无欢意。E\n",
      "Obs:\n",
      "情未已，暮雨难将息。叶底揽花痕，并蒂瘦、娇无欢意。E\n",
      "Rnd:\n",
      "几恨酣游春殿，来不撩雨霜上年。一翠禅阳遥隐夜人雪雪江云晚去雁心寂我\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-259-11be32a53996>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mLoss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdisp_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/torch/optim/adam.pyc\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                     \u001b[0;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lr = 1e-3\n",
    "n_epochs = 10000\n",
    "last_epoch = -1\n",
    "disp_interval = 50\n",
    "batch_size = 1\n",
    "\n",
    "loss_func = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n",
    "\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "def lr_lambda(epoch):\n",
    "    return 0.99**(epoch/50.0)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch=last_epoch)\n",
    "\n",
    "model.load_state_dict(torch.load('saves/model-manyun.pt'))\n",
    "\n",
    "Loss = []\n",
    "for epoch in range(n_epochs):\n",
    "    model.zero_grad()\n",
    "    x_obs, y_obs = get_batch(batch_size=batch_size)\n",
    "    x_obs = idx2emb(x_obs)\n",
    "    y_pred, ht = model(x_obs)\n",
    "    y1 = torch.argmax(y_pred.detach(),-1,keepdim=True).detach()#[:,:1,:]\n",
    "    y2 = y_obs.detach()#[:,:1,:]\n",
    "    y_pred = y_pred.view(-1,output_size)\n",
    "    y_obs = y_obs.contiguous().view(-1)\n",
    "    loss = loss_func(y_pred,y_obs)\n",
    "    loss.backward()\n",
    "    Loss.append(loss.tolist())\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    if epoch % disp_interval == 0:\n",
    "        print(u'Epoch{}, Loss{}\\nPred:\\n{}\\nObs:\\n{}\\nRnd:\\n{}\\n'.format(epoch,loss.tolist(), batch2sent(y1), batch2sent(y2),poem_gen(model)))\n",
    "        torch.save(model.state_dict(),'saves/model-manyun.pt')\n",
    "window_size = 50\n",
    "avg_losses = np.array(Loss)[:len(Loss)//50 *50].reshape([-1,window_size]).mean(1)\n",
    "pl.plot(np.arange(0,len(Loss)//50 *50,window_size), avg_losses,'r-')\n",
    "pl.xlabel('Time')\n",
    "pl.ylabel('Loss')\n",
    "pl.yscale('log')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "荷池台上救散，太日梅雪谁？无意行强欢尽。一朝万里事客雪。知看多禅及心公\n"
     ]
    }
   ],
   "source": [
    "print poem_gen(model,u'荷')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2,3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25, 5, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_obs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.transpose(input,1,0).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.topk(torch.randn(1,1,100),1,-1)[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.binomial(6,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 6.,  6.,  4.,  0.],\n",
      "        [ 4.,  0.,  8.,  2.],\n",
      "        [ 0.,  8.,  9.,  2.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randint(0,10,[3,4])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 128])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2emb(torch.LongTensor([[[ 236]]])\n",
    ").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2],\n",
       "        [ 2,  0,  3],\n",
       "        [ 2,  1,  3]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.topk(x,3,dim=-1)[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入模块\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "from torch import nn\n",
    "\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个简单RNN层\n",
    "class ERNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(ERNN,self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.ih_linear = nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.hh_linear = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        \n",
    "    def init_h(self, x):\n",
    "        self.ht = torch.randn_like(x[0])\n",
    "    \n",
    "    def forward(self, x, h=None):\n",
    "        if h is None:\n",
    "            self.init_h(x)\n",
    "        seq_length, batch_size, input_size = x.size()\n",
    "        y = []\n",
    "        for t in range(seq_length):\n",
    "            self.ht = torch.tanh(self.ih_linear(x[t]) + self.hh_linear(self.ht))\n",
    "            y.append(self.ht.unsqueeze(0))\n",
    "        y = torch.cat(y)\n",
    "        return y, self.ht\n",
    "\n",
    "# 定义一个LSTM层\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(LSTM,self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.ii_linear = nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.hi_linear = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.if_linear = nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.hf_linear = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.ig_linear = nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.hg_linear = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.io_linear = nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.ho_linear = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "    def init_h(self, x):\n",
    "        self.ht = torch.randn_like(x[0])\n",
    "    def init_c(self, x):\n",
    "        self.ct = torch.randn_like(x[0])\n",
    "    \n",
    "    def forward(self, x, h=None, c=None):\n",
    "        if h is None:\n",
    "            self.init_h(x)\n",
    "        if c is None:\n",
    "            self.init_c(x)\n",
    "        seq_length, batch_size, input_size = x.size()\n",
    "        y = []\n",
    "        for t in range(seq_length):\n",
    "            it = torch.sigmoid(self.ii_linear(x[t]) + self.hi_linear(self.ht))\n",
    "            ft = torch.sigmoid(self.if_linear(x[t]) + self.hf_linear(self.ht))\n",
    "            gt = torch.tanh(self.ig_linear(x[t]) + self.hg_linear(self.ht))\n",
    "            ot = torch.sigmoid(self.io_linear(x[t]) + self.ho_linear(self.ht))\n",
    "            self.ct = ft * self.ct + it * gt\n",
    "            self.ht = ot * torch.tanh(self.ct)\n",
    "            y.append(self.ht.unsqueeze(0))\n",
    "        y = torch.cat(y)\n",
    "        return y, self.ht\n",
    "\n",
    "    \n",
    "# 定义一个GRU层\n",
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(GRU,self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.in_linear = nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.hn_linear = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.ir_linear = nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.hr_linear = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.iz_linear = nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.hz_linear = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "    def init_h(self, x):\n",
    "        self.ht = torch.randn_like(x[0])\n",
    "    \n",
    "    def forward(self, x, h=None):\n",
    "        if h is None:\n",
    "            self.init_h(x)\n",
    "        seq_length, batch_size, input_size = x.size()\n",
    "        y = []\n",
    "        for t in range(seq_length):\n",
    "            rt = torch.sigmoid(self.ir_linear(x[t]) + self.hr_linear(self.ht))\n",
    "            zt = torch.sigmoid(self.iz_linear(x[t]) + self.hz_linear(self.ht))\n",
    "            nt = torch.tanh(self.in_linear(x[t]) + rt * self.hn_linear(self.ht))\n",
    "            self.ht = (1 - zt) * nt + zt * self.ht\n",
    "            y.append(self.ht.unsqueeze(0))\n",
    "        y = torch.cat(y)\n",
    "        return y, self.ht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data summary:\n",
      "\n",
      " number of poems: 42\n",
      " number of words: 773\n",
      "\n",
      "Poem examples:\n",
      "\n",
      "S平生何所寄？天地一孤篷。郁纡且行游，迟复尘景中。E\n",
      "S星汉奔岩屿，惊涛卷曈虹。翕趿隐烟色，长桥海岛空。E\n",
      "S百年如云梦，逆旅何匆匆。吟坐忘知闻，拈花鉴溟濛。E\n",
      "S道心不外求，日影养虚冲。观风遣剑意，抱朴任穷通。E\n",
      "S千古一杯清，卧剑亦何如？云雁有芳信，谈笑未成书。E\n",
      "S故国弛山色，春华因才逐。北庭惜玉折，积风待岁除。E\n",
      "S俯仰苍茫间，太虚应有诸。值此吟月夜，借居怀纡余。E\n",
      "S心斋即坛醮，守道安违俗。江湖得意气，狂歌岂踟躇。E\n",
      "S所忧非尘辙，萧萧演六虚。冷眼任霜雪，平生性慵疏。E\n",
      "S浮景或可悲，愁予感韫椟。明日放归去，长梦酬三馀。E\n"
     ]
    }
   ],
   "source": [
    "# 训练一个基于ERNN神经网络来作诗\n",
    "\n",
    "## 读入用GloVe处理得到的文字 embeddings，以及句子数据。\n",
    "import codecs\n",
    "\n",
    "with codecs.open('data/word_embeddings_manyun_128.txt', mode='r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "n_words = len(lines) + 1\n",
    "word_emb_dim = input_size = 128\n",
    "word_embeddings = torch.nn.Embedding(n_words, word_emb_dim)\n",
    "i2w = {0:''}\n",
    "w2i = {'':0}\n",
    "for i in range(0, n_words - 1):\n",
    "    line = lines[i].split(' ')\n",
    "    i2w[i + 1] = line[0]\n",
    "    w2i[line[0]] = i + 1\n",
    "#    word_embeddings.weight[i] = torch.from_numpy(np.array(line[1:],dtype=np.float32))\n",
    "\n",
    "word_embeddings.weight.require_grad = False\n",
    "\n",
    "poems = []\n",
    "max_line_length = 32\n",
    "with codecs.open('data/manyun.txt', mode='r', encoding='utf-8') as f:\n",
    "    for poem in f:\n",
    "        poem = poem.replace(' ','')\n",
    "        if ':' in poem: poem = poem.split(':')[-1]\n",
    "        poem = poem.replace('\\n','')\n",
    "        poem = poem.replace('\\r','')\n",
    "        if len(poem) < 24 or len(poem) > max_line_length or '(' in poem or u'（' in poem or u'）' in poem or ')' in poem:\n",
    "            continue\n",
    "        poem = 'S' + poem + 'E'\n",
    "        poems.append(map(w2i.get, poem))\n",
    "\n",
    "n_poems = len(poems)\n",
    "\n",
    "print( 'Data summary:\\n\\n number of poems: {}\\n number of words: {}\\n'.format(n_poems, n_words))\n",
    "print('Poem examples:\\n\\n'+'\\n'.join([''.join(map(i2w.get, x)) for x in poems[:10]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S常怜香烬，终日成孤倚。忍泪付新杯，醉时看、飞云化碧。\n",
      "常怜香烬，终日成孤倚。忍泪付新杯，醉时看、飞云化碧。E\n"
     ]
    }
   ],
   "source": [
    "# 定义一个函数，随机返回一个 mini batch，用于训练，由于每一首诗歌的长度不同，我们此处规定每个batch只有一首诗。这样，就可以生成长度可变的诗歌。\n",
    "def get_batch(batch_size=1):\n",
    "    batch_raw = [poems[i][:] for i in np.random.randint(0, n_poems, batch_size)]\n",
    "    max_length = max(map(len, batch_raw))\n",
    "    for i in range(len(batch_raw)):\n",
    "        for j in range(len(batch_raw[i]),max_length):\n",
    "            batch_raw[i].append(w2i[''])\n",
    "    batch_raw = torch.LongTensor(batch_raw).detach().unsqueeze(2).transpose(0,1)\n",
    "    x = batch_raw[:-1].type(torch.float32)\n",
    "    y = batch_raw[1:]\n",
    "    return x, y\n",
    "\n",
    "def idx2emb(x):\n",
    "    return word_embeddings(x.type(torch.long)).squeeze(2).detach()\n",
    "    \n",
    "\n",
    "# 定义一个函数，输入一个 batch 返回句子\n",
    "def batch2sent(batch):\n",
    "    S = []\n",
    "    batch = batch.type(torch.int32).detach()\n",
    "    seq_length, batch_size, emb_size = batch.size()\n",
    "    for i in range(batch_size):\n",
    "        S.append(''.join(map(i2w.get, batch[:,i,:].view(-1).tolist())))\n",
    "    return u'\\n'.join(S)\n",
    "\n",
    "x, y = get_batch(1)\n",
    "print(batch2sent(x))\n",
    "print(batch2sent(y))\n",
    "\n",
    "# 定义一个生成器\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = LSTM(self.input_size, self.hidden_size)\n",
    "        self.output = nn.Linear(self.hidden_size, self.output_size)\n",
    "        self.logsoftmax = torch.nn.LogSoftmax(dim=-1)\n",
    "    def forward(self, x, h0=None):\n",
    "        seq_length, batch_size, input_size = x.size()\n",
    "        y, ht = self.rnn(x, h0)\n",
    "        y = y.view(-1, self.hidden_size)\n",
    "        y = self.output(y)\n",
    "        y = y.view(seq_length, batch_size, output_size)\n",
    "        y = self.logsoftmax(y)\n",
    "        return y, ht\n",
    "\n",
    "def poem_gen(model, w=None, cr=1e-1):\n",
    "    with torch.no_grad():\n",
    "        if not w in w2i or w is None:\n",
    "            idx = np.random.randint(1,n_words)\n",
    "            w = i2w[idx]\n",
    "        else:\n",
    "            idx = w2i[w]\n",
    "        ht = None\n",
    "        x0 = torch.FloatTensor([w2i['S']]).view(1,1,-1).detach()\n",
    "        x0 = idx2emb(x0)\n",
    "        y, ht = model(x0, ht)\n",
    "        x = torch.LongTensor([w2i[w]]).view(1,1,-1).detach()\n",
    "        x = idx2emb(x)\n",
    "        s = []\n",
    "        s.append(w)\n",
    "        for t in range(max_line_length):\n",
    "            y, ht = model(x, ht)\n",
    "            not_done = True\n",
    "            cnt = 0\n",
    "            while not_done and cnt <50:\n",
    "                k = min([1+np.random.binomial(3,0.5), y.size(-1)-1])\n",
    "                x = torch.topk(y, k, dim=-1)[1].detach()\n",
    "                x = x[:,:,min([np.random.geometric(0.3), k-1])].unsqueeze(2)\n",
    "#                x = torch.argmax(y,dim=-1,keepdim=True)\n",
    "                cnt += 1\n",
    "                w = batch2sent(x)\n",
    "                not_done = False\n",
    "            if w == 'E':\n",
    "                break\n",
    "            s.append(w)\n",
    "            x = idx2emb(x)\n",
    "        return u''.join(s)\n",
    "    \n",
    "    \n",
    "# 训练一个简单的 RNN 模型以生成诗歌\n",
    "\n",
    "input_size = word_emb_dim\n",
    "hidden_size = 128\n",
    "output_size = n_words\n",
    "\n",
    "model = Generator(input_size, output_size, hidden_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'ct' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-f567843ef43d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mx_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_obs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mx_obs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx2emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_obs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mht\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_obs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0my1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#[:,:1,:]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0my2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_obs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#[:,:1,:]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-62-b2a9509efe75>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, h0)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mht\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-60-809f189d37d5>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, h, c)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio_linear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mho_linear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mht\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mft\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mct\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mit\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mht\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mot\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mht\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'ct' is not defined"
     ]
    }
   ],
   "source": [
    "lr = 1e-3\n",
    "n_epochs = 20000\n",
    "last_epoch = -1\n",
    "disp_interval = 50\n",
    "batch_size = 2\n",
    "\n",
    "loss_func = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n",
    "\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "def lr_lambda(epoch):\n",
    "    return 0.99**(epoch/50.0)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch=last_epoch)\n",
    "\n",
    "#model.load_state_dict(torch.load('saves/model-ernn.pt'))\n",
    "\n",
    "Loss = []\n",
    "for epoch in range(n_epochs):\n",
    "    model.zero_grad()\n",
    "    x_obs, y_obs = get_batch(batch_size=batch_size)\n",
    "    x_obs = idx2emb(x_obs)\n",
    "    y_pred, ht = model(x_obs)\n",
    "    y1 = torch.argmax(y_pred.detach(),-1,keepdim=True).detach()#[:,:1,:]\n",
    "    y2 = y_obs.detach()#[:,:1,:]\n",
    "    y_pred = y_pred.view(-1,output_size)\n",
    "    y_obs = y_obs.contiguous().view(-1)\n",
    "    loss = loss_func(y_pred,y_obs)\n",
    "    loss.backward()\n",
    "    Loss.append(loss.tolist())\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    if epoch % disp_interval == 0:\n",
    "        print(u'Epoch{}, Loss{}\\nPred:\\n{}\\nObs:\\n{}\\nRnd:\\n{}\\n'.format(epoch,loss.tolist(), batch2sent(y1), batch2sent(y2),poem_gen(model)))\n",
    "        torch.save(model.state_dict(),'saves/model-ernn.pt')\n",
    "window_size = 50\n",
    "avg_losses = np.array(Loss)[:len(Loss)//50 *50].reshape([-1,window_size]).mean(1)\n",
    "pl.plot(np.arange(0,len(Loss)//50 *50,window_size), avg_losses,'r-')\n",
    "pl.xlabel('Time')\n",
    "pl.ylabel('Loss')\n",
    "pl.yscale('log')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1807,  0.6392,  0.1268,  1.1103, -1.6153, -0.1511, -0.0435,\n",
       "        -0.0471, -0.8821, -0.4894, -0.1626,  0.1210,  1.6830, -0.1591,\n",
       "        -0.0498, -0.0988,  0.3077,  1.0061,  0.2484, -0.7002])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embeddings.weight[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "252450"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "33*7650"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 3])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([torch.zeros(4,3).unsqueeze(0),torch.zeros(4,3).unsqueeze(0)]).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "16661700/7650/33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
